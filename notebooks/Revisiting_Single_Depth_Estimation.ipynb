{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Revisiting_Single_Depth_Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2XsAGv9UOv9"
      },
      "source": [
        "Code borrowed from : https://github.com/JunjH/Revisiting_Single_Depth_Estimation/tree/a486b9a3fc708a4f320881ee3b31f94022e1c184 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se6p5YPXTwuO",
        "outputId": "67f5c03b-8596-4af1-9300-b80ac72f752e"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "py_file_location = \"/content/drive/My Drive/Revisiting_Single_Depth_Estimation\"\n",
        "sys.path.append(py_file_location)\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "import collections\n",
        "\n",
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9xNuuNUWUQd",
        "outputId": "4b286885-de08-48a8-8e10-4b81d2a9fd8b"
      },
      "source": [
        "cd '/content/drive/My Drive/Revisiting_Single_Depth_Estimation/'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Revisiting_Single_Depth_Estimation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2ejYFpdmxga1",
        "outputId": "a83f831f-ea5a-4637-af90-b190e14b4d6c"
      },
      "source": [
        "import torch\n",
        "import torch.nn.parallel\n",
        "\n",
        "# import senet\n",
        "# import modules\n",
        "# import resnet\n",
        "# import net\n",
        "# import densenet\n",
        "\n",
        "import matplotlib.image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap(\"jet\")\n",
        "from PIL import Image"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m83aejXFf3GU"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils import model_zoo\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n",
        "           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n",
        "\n",
        "pretrained_settings = {\n",
        "    'senet154': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 224, 224],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225],\n",
        "            'num_classes': 1000\n",
        "        }\n",
        "    },\n",
        "    'se_resnet50': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 224, 224],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225],\n",
        "            'num_classes': 1000\n",
        "        }\n",
        "    },\n",
        "    'se_resnet101': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 224, 224],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225],\n",
        "            'num_classes': 1000\n",
        "        }\n",
        "    },\n",
        "    'se_resnet152': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 224, 224],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225],\n",
        "            'num_classes': 1000\n",
        "        }\n",
        "    },\n",
        "    'se_resnext50_32x4d': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 224, 224],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225],\n",
        "            'num_classes': 1000\n",
        "        }\n",
        "    },\n",
        "    'se_resnext101_32x4d': {\n",
        "        'imagenet': {\n",
        "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n",
        "            'input_space': 'RGB',\n",
        "            'input_size': [3, 224, 224],\n",
        "            'input_range': [0, 1],\n",
        "            'mean': [0.485, 0.456, 0.406],\n",
        "            'std': [0.229, 0.224, 0.225],\n",
        "            'num_classes': 1000\n",
        "        }\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "class SEModule(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, reduction):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
        "                             padding=0)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
        "                             padding=0)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return module_input * x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for bottlenecks that implements `forward()` method.\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out = self.se_module(out) + residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEBottleneck(Bottleneck):\n",
        "    \"\"\"\n",
        "    Bottleneck for SENet154.\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
        "                 downsample=None):\n",
        "        super(SEBottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
        "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=groups,\n",
        "                               bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
        "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "class SEResNetBottleneck(Bottleneck):\n",
        "    \"\"\"\n",
        "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
        "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
        "    (the latter is used in the torchvision implementation of ResNet).\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
        "                 downsample=None):\n",
        "        super(SEResNetBottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
        "                               stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
        "                               groups=groups, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "class SEResNeXtBottleneck(Bottleneck):\n",
        "    \"\"\"\n",
        "    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
        "                 downsample=None, base_width=4):\n",
        "        super(SEResNeXtBottleneck, self).__init__()\n",
        "        # width = math.floor(planes * (base_width / 64)) * groups\n",
        "        # pdb.set_trace()\n",
        "        width = int(planes * base_width / 64) * groups\n",
        "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n",
        "                               stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(width)\n",
        "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
        "                               padding=1, groups=groups, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(width)\n",
        "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "class SENet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
        "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
        "                 downsample_padding=1, num_classes=1000):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        block (nn.Module): Bottleneck class.\n",
        "            - For SENet154: SEBottleneck\n",
        "            - For SE-ResNet models: SEResNetBottleneck\n",
        "            - For SE-ResNeXt models:  SEResNeXtBottleneck\n",
        "        layers (list of ints): Number of residual blocks for 4 layers of the\n",
        "            network (layer1...layer4).\n",
        "        groups (int): Number of groups for the 3x3 convolution in each\n",
        "            bottleneck block.\n",
        "            - For SENet154: 64\n",
        "            - For SE-ResNet models: 1\n",
        "            - For SE-ResNeXt models:  32\n",
        "        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n",
        "            - For all models: 16\n",
        "        dropout_p (float or None): Drop probability for the Dropout layer.\n",
        "            If `None` the Dropout layer is not used.\n",
        "            - For SENet154: 0.2\n",
        "            - For SE-ResNet models: None\n",
        "            - For SE-ResNeXt models: None\n",
        "        inplanes (int):  Number of input channels for layer1.\n",
        "            - For SENet154: 128\n",
        "            - For SE-ResNet models: 64\n",
        "            - For SE-ResNeXt models: 64\n",
        "        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n",
        "            a single 7x7 convolution in layer0.\n",
        "            - For SENet154: True\n",
        "            - For SE-ResNet models: False\n",
        "            - For SE-ResNeXt models: False\n",
        "        downsample_kernel_size (int): Kernel size for downsampling convolutions\n",
        "            in layer2, layer3 and layer4.\n",
        "            - For SENet154: 3\n",
        "            - For SE-ResNet models: 1\n",
        "            - For SE-ResNeXt models: 1\n",
        "        downsample_padding (int): Padding for downsampling convolutions in\n",
        "            layer2, layer3 and layer4.\n",
        "            - For SENet154: 1\n",
        "            - For SE-ResNet models: 0\n",
        "            - For SE-ResNeXt models: 0\n",
        "        num_classes (int): Number of outputs in `last_linear` layer.\n",
        "            - For all models: 1000\n",
        "        \"\"\"\n",
        "        super(SENet, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        if input_3x3:\n",
        "            layer0_modules = [\n",
        "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
        "                                    bias=False)),\n",
        "                ('bn1', nn.BatchNorm2d(64)),\n",
        "                ('relu1', nn.ReLU(inplace=True)),\n",
        "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
        "                                    bias=False)),\n",
        "                ('bn2', nn.BatchNorm2d(64)),\n",
        "                ('relu2', nn.ReLU(inplace=True)),\n",
        "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
        "                                    bias=False)),\n",
        "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
        "                ('relu3', nn.ReLU(inplace=True)),\n",
        "            ]\n",
        "        else:\n",
        "            layer0_modules = [\n",
        "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
        "                                    padding=3, bias=False)),\n",
        "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
        "                ('relu1', nn.ReLU(inplace=True)),\n",
        "            ]\n",
        "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
        "        # is used instead of `padding=1`.\n",
        "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
        "                                                    ceil_mode=True)))\n",
        "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
        "        self.layer1 = self._make_layer(\n",
        "            block,\n",
        "            planes=64,\n",
        "            blocks=layers[0],\n",
        "            groups=groups,\n",
        "            reduction=reduction,\n",
        "            downsample_kernel_size=1,\n",
        "            downsample_padding=0\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block,\n",
        "            planes=128,\n",
        "            blocks=layers[1],\n",
        "            stride=2,\n",
        "            groups=groups,\n",
        "            reduction=reduction,\n",
        "            downsample_kernel_size=downsample_kernel_size,\n",
        "            downsample_padding=downsample_padding\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block,\n",
        "            planes=256,\n",
        "            blocks=layers[2],\n",
        "            stride=2,\n",
        "            groups=groups,\n",
        "            reduction=reduction,\n",
        "            downsample_kernel_size=downsample_kernel_size,\n",
        "            downsample_padding=downsample_padding\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block,\n",
        "            planes=512,\n",
        "            blocks=layers[3],\n",
        "            stride=2,\n",
        "            groups=groups,\n",
        "            reduction=reduction,\n",
        "            downsample_kernel_size=downsample_kernel_size,\n",
        "            downsample_padding=downsample_padding\n",
        "        )\n",
        "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
        "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
        "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
        "                    downsample_kernel_size=1, downsample_padding=0):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=downsample_kernel_size, stride=stride,\n",
        "                          padding=downsample_padding, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
        "                            downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def features(self, x):\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        " \n",
        "        return x\n",
        "\n",
        "\n",
        "    def logits(self, x):\n",
        "        x = self.avg_pool(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.last_linear(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x,x_):      \n",
        "        x = self.features(x)\n",
        "        x = self.logits(x)\n",
        "        return x\n",
        "\n",
        "def initialize_pretrained_model(model, num_classes, settings):\n",
        "    assert num_classes == settings['num_classes'], \\\n",
        "        'num_classes should be {}, but is {}'.format(\n",
        "            settings['num_classes'], num_classes)\n",
        "    model.load_state_dict(model_zoo.load_url(settings['url'], 'pretrained_model/encoder'))\n",
        "    model.input_space = settings['input_space']\n",
        "    model.input_size = settings['input_size']\n",
        "    model.input_range = settings['input_range']\n",
        "    model.mean = settings['mean']\n",
        "    model.std = settings['std']\n",
        "\n",
        "\n",
        "def senet154(num_classes=1000, pretrained='imagenet'):\n",
        "    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n",
        "                  dropout_p=0.2, num_classes=num_classes)\n",
        "    if pretrained is not None:\n",
        "        settings = pretrained_settings['senet154'][pretrained]\n",
        "        initialize_pretrained_model(model, num_classes, settings)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet50(num_classes=1000, pretrained='imagenet'):\n",
        "    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n",
        "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
        "                  downsample_kernel_size=1, downsample_padding=0,\n",
        "                  num_classes=num_classes)\n",
        "    if pretrained is not None:\n",
        "        settings = pretrained_settings['se_resnet50'][pretrained]\n",
        "        initialize_pretrained_model(model, num_classes, settings)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet101(num_classes=1000, pretrained='imagenet'):\n",
        "    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n",
        "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
        "                  downsample_kernel_size=1, downsample_padding=0,\n",
        "                  num_classes=num_classes)\n",
        "    if pretrained is not None:\n",
        "        settings = pretrained_settings['se_resnet101'][pretrained]\n",
        "        initialize_pretrained_model(model, num_classes, settings)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnet152(num_classes=1000, pretrained='imagenet'):\n",
        "    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n",
        "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
        "                  downsample_kernel_size=1, downsample_padding=0,\n",
        "                  num_classes=num_classes)\n",
        "    if pretrained is not None:\n",
        "        settings = pretrained_settings['se_resnet152'][pretrained]\n",
        "        initialize_pretrained_model(model, num_classes, settings)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n",
        "    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
        "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
        "                  downsample_kernel_size=1, downsample_padding=0,\n",
        "                  num_classes=num_classes)\n",
        "    if pretrained is not None:\n",
        "        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n",
        "        initialize_pretrained_model(model, num_classes, settings)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n",
        "    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n",
        "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
        "                  downsample_kernel_size=1, downsample_padding=0,\n",
        "                  num_classes=num_classes)\n",
        "    if pretrained is not None:\n",
        "        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n",
        "        initialize_pretrained_model(model, num_classes, settings)\n",
        "    return model\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ezQJTQhYir"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils import model_zoo\n",
        "import copy\n",
        "import numpy as np\n",
        "# import senet\n",
        "# import resnet\n",
        "# import densenet\n",
        "\n",
        "class _UpProjection(nn.Sequential):\n",
        "\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_UpProjection, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_input_features, num_output_features,\n",
        "                               kernel_size=5, stride=1, padding=2, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(num_output_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(num_output_features, num_output_features,\n",
        "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1_2 = nn.BatchNorm2d(num_output_features)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_input_features, num_output_features,\n",
        "                               kernel_size=5, stride=1, padding=2, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(num_output_features)\n",
        "\n",
        "    def forward(self, x, size):\n",
        "        x = F.upsample(x, size=size, mode='bilinear')\n",
        "        x_conv1 = self.relu(self.bn1(self.conv1(x)))\n",
        "        bran1 = self.bn1_2(self.conv1_2(x_conv1))\n",
        "        bran2 = self.bn2(self.conv2(x))\n",
        "\n",
        "        out = self.relu(bran1 + bran2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class E_resnet(nn.Module):\n",
        "\n",
        "    def __init__(self, original_model, num_features = 2048):\n",
        "        super(E_resnet, self).__init__()        \n",
        "        self.conv1 = original_model.conv1\n",
        "        self.bn1 = original_model.bn1\n",
        "        self.relu = original_model.relu\n",
        "        self.maxpool = original_model.maxpool\n",
        "\n",
        "        self.layer1 = original_model.layer1\n",
        "        self.layer2 = original_model.layer2\n",
        "        self.layer3 = original_model.layer3\n",
        "        self.layer4 = original_model.layer4\n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x_block1 = self.layer1(x)\n",
        "        x_block2 = self.layer2(x_block1)\n",
        "        x_block3 = self.layer3(x_block2)\n",
        "        x_block4 = self.layer4(x_block3)\n",
        "\n",
        "        return x_block1, x_block2, x_block3, x_block4\n",
        "\n",
        "class E_densenet(nn.Module):\n",
        "\n",
        "    def __init__(self, original_model, num_features = 2208):\n",
        "        super(E_densenet, self).__init__()        \n",
        "        self.features = original_model.features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x01 = self.features[0](x)\n",
        "        x02 = self.features[1](x01)\n",
        "        x03 = self.features[2](x02)\n",
        "        x04 = self.features[3](x03)\n",
        "\n",
        "        x_block1 = self.features[4](x04)\n",
        "        x_block1 = self.features[5][0](x_block1)\n",
        "        x_block1 = self.features[5][1](x_block1)\n",
        "        x_block1 = self.features[5][2](x_block1)\n",
        "        x_tran1 = self.features[5][3](x_block1)\n",
        "\n",
        "        x_block2 = self.features[6](x_tran1)\n",
        "        x_block2 = self.features[7][0](x_block2)\n",
        "        x_block2 = self.features[7][1](x_block2)\n",
        "        x_block2 = self.features[7][2](x_block2)\n",
        "        x_tran2 = self.features[7][3](x_block2)\n",
        "\n",
        "        x_block3 = self.features[8](x_tran2)\n",
        "        x_block3 = self.features[9][0](x_block3)\n",
        "        x_block3 = self.features[9][1](x_block3)\n",
        "        x_block3 = self.features[9][2](x_block3)\n",
        "        x_tran3 = self.features[9][3](x_block3)\n",
        "\n",
        "        x_block4 = self.features[10](x_tran3)\n",
        "        x_block4 = F.relu(self.features[11](x_block4))\n",
        "\n",
        "        return x_block1, x_block2, x_block3, x_block4\n",
        "\n",
        "class E_senet(nn.Module):\n",
        "\n",
        "    def __init__(self, original_model, num_features = 2048):\n",
        "        super(E_senet, self).__init__()        \n",
        "        self.base = nn.Sequential(*list(original_model.children())[:-3])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base[0](x)\n",
        "        x_block1 = self.base[1](x)\n",
        "        x_block2 = self.base[2](x_block1)\n",
        "        x_block3 = self.base[3](x_block2)\n",
        "        x_block4 = self.base[4](x_block3)\n",
        "\n",
        "        return x_block1, x_block2, x_block3, x_block4\n",
        "\n",
        "class D(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features = 2048):\n",
        "        super(D, self).__init__()\n",
        "        self.conv = nn.Conv2d(num_features, num_features //\n",
        "                               2, kernel_size=1, stride=1, bias=False)\n",
        "        num_features = num_features // 2\n",
        "        self.bn = nn.BatchNorm2d(num_features)\n",
        "\n",
        "        self.up1 = _UpProjection(\n",
        "            num_input_features=num_features, num_output_features=num_features // 2)\n",
        "        num_features = num_features // 2\n",
        "\n",
        "        self.up2 = _UpProjection(\n",
        "            num_input_features=num_features, num_output_features=num_features // 2)\n",
        "        num_features = num_features // 2\n",
        "\n",
        "        self.up3 = _UpProjection(\n",
        "            num_input_features=num_features, num_output_features=num_features // 2)\n",
        "        num_features = num_features // 2\n",
        "\n",
        "        self.up4 = _UpProjection(\n",
        "            num_input_features=num_features, num_output_features=num_features // 2)\n",
        "        num_features = num_features // 2\n",
        "\n",
        "\n",
        "    def forward(self, x_block1, x_block2, x_block3, x_block4):\n",
        "        x_d0 = F.relu(self.bn(self.conv(x_block4)))\n",
        "        x_d1 = self.up1(x_d0, [x_block3.size(2), x_block3.size(3)])\n",
        "        x_d2 = self.up2(x_d1, [x_block2.size(2), x_block2.size(3)])\n",
        "        x_d3 = self.up3(x_d2, [x_block1.size(2), x_block1.size(3)])\n",
        "        x_d4 = self.up4(x_d3, [x_block1.size(2)*2, x_block1.size(3)*2])\n",
        "\n",
        "        return x_d4\n",
        "\n",
        "class MFF(nn.Module):\n",
        "\n",
        "    def __init__(self, block_channel, num_features=64):\n",
        "\n",
        "        super(MFF, self).__init__()\n",
        "        \n",
        "        self.up1 = _UpProjection(\n",
        "            num_input_features=block_channel[0], num_output_features=16)\n",
        "        \n",
        "        self.up2 = _UpProjection(\n",
        "            num_input_features=block_channel[1], num_output_features=16)\n",
        "       \n",
        "        self.up3 = _UpProjection(\n",
        "            num_input_features=block_channel[2], num_output_features=16)\n",
        "       \n",
        "        self.up4 = _UpProjection(\n",
        "            num_input_features=block_channel[3], num_output_features=16)\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            num_features, num_features, kernel_size=5, stride=1, padding=2, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(num_features)\n",
        "        \n",
        "\n",
        "    def forward(self, x_block1, x_block2, x_block3, x_block4, size):\n",
        "        x_m1 = self.up1(x_block1, size)\n",
        "        x_m2 = self.up2(x_block2, size)\n",
        "        x_m3 = self.up3(x_block3, size)\n",
        "        x_m4 = self.up4(x_block4, size)\n",
        "\n",
        "        x = self.bn(self.conv(torch.cat((x_m1, x_m2, x_m3, x_m4), 1)))\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class R(nn.Module):\n",
        "    def __init__(self, block_channel):\n",
        "\n",
        "        super(R, self).__init__()\n",
        "        \n",
        "        num_features = 64 + block_channel[3]//32\n",
        "        self.conv0 = nn.Conv2d(num_features, num_features,\n",
        "                               kernel_size=5, stride=1, padding=2, bias=False)\n",
        "        self.bn0 = nn.BatchNorm2d(num_features)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_features, num_features,\n",
        "                               kernel_size=5, stride=1, padding=2, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            num_features, 1, kernel_size=5, stride=1, padding=2, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.conv0(x)\n",
        "        x0 = self.bn0(x0)\n",
        "        x0 = F.relu(x0)\n",
        "\n",
        "        x1 = self.conv1(x0)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = F.relu(x1)\n",
        "\n",
        "        x2 = self.conv2(x1)\n",
        "\n",
        "        return x2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhJKTBythhmH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# import modules\n",
        "\n",
        "\n",
        "# import senet\n",
        "# import resnet\n",
        "# import densenet\n",
        "\n",
        "class model_enc(nn.Module):\n",
        "    def __init__(self, Encoder, num_features, block_channel):\n",
        "\n",
        "        super(model_enc, self).__init__()\n",
        "\n",
        "        self.E = Encoder\n",
        "        self.D = D(num_features)\n",
        "        self.MFF = MFF(block_channel)\n",
        "        self.R = R(block_channel)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_block1, x_block2, x_block3, x_block4 = self.E(x)\n",
        "        x_decoder = self.D(x_block1, x_block2, x_block3, x_block4)\n",
        "        x_mff = self.MFF(x_block1, x_block2, x_block3, x_block4,[x_decoder.size(2),x_decoder.size(3)])\n",
        "        out = self.R(torch.cat((x_decoder, x_mff), 1))\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f_FmLoxKZ6m"
      },
      "source": [
        "def normalize_batch(batch):\n",
        "    '''Normalize a tensor in [0,1] using imagenet mean and std'''    \n",
        "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "    return (batch - mean) / std"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSg9oaUKUsW8"
      },
      "source": [
        "def _is_pil_image(img):\n",
        "    if accimage is not None:\n",
        "        return isinstance(img, (Image.Image, accimage.Image))\n",
        "    else:\n",
        "        return isinstance(img, Image.Image)\n",
        "\n",
        "\n",
        "def _is_numpy_image(img):\n",
        "    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n",
        "\n",
        "class Scale(object):\n",
        "    \"\"\" Rescales the inputs and target arrays to the given 'size'.\n",
        "    'size' will be the size of the smaller edge.\n",
        "    For example, if height > width, then image will be\n",
        "    rescaled to (size * height / width, size)\n",
        "    size: size of the smaller edge\n",
        "    interpolation order: Default: 2 (bilinear)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "\n",
        "        image = self.changeScale(image, self.size)\n",
        "        depth = self.changeScale(depth, self.size,Image.NEAREST)\n",
        " \n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def changeScale(self, img, size, interpolation=Image.BILINEAR):\n",
        "\n",
        "        if not _is_pil_image(img):\n",
        "            raise TypeError(\n",
        "                'img should be PIL Image. Got {}'.format(type(img)))\n",
        "        if not (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)):\n",
        "            raise TypeError('Got inappropriate size arg: {}'.format(size))\n",
        "\n",
        "        if isinstance(size, int):\n",
        "            w, h = img.size\n",
        "            if (w <= h and w == size) or (h <= w and h == size):\n",
        "                return img\n",
        "            if w < h:\n",
        "                ow = size\n",
        "                oh = int(size * h / w)\n",
        "                return img.resize((ow, oh), interpolation)\n",
        "            else:\n",
        "                oh = size\n",
        "                ow = int(size * w / h)\n",
        "                return img.resize((ow, oh), interpolation)\n",
        "        else:\n",
        "            return img.resize(size[::-1], interpolation)\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size_image, size_depth):\n",
        "        self.size_image = size_image\n",
        "        self.size_depth = size_depth\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "\n",
        "        image = self.centerCrop(image, self.size_image)\n",
        "        depth = self.centerCrop(depth, self.size_image)\n",
        "\n",
        "        ow, oh = self.size_depth\n",
        "        depth = depth.resize((ow, oh))\n",
        "\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def centerCrop(self, image, size):\n",
        "\n",
        "        w1, h1 = image.size\n",
        "\n",
        "        tw, th = size\n",
        "\n",
        "        if w1 == tw and h1 == th:\n",
        "            return image\n",
        "\n",
        "        x1 = int(round((w1 - tw) / 2.))\n",
        "        y1 = int(round((h1 - th) / 2.))\n",
        "\n",
        "        image = image.crop((x1, y1, tw + x1, th + y1))\n",
        "\n",
        "        return image\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
        "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    \"\"\"\n",
        "    def __init__(self,is_test=False):\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        # ground truth depth of training samples is stored in 8-bit while test samples are saved in 16 bit\n",
        "        image = self.to_tensor(image)\n",
        "        if self.is_test:\n",
        "            depth = self.to_tensor(depth).float()/1000\n",
        "        else:            \n",
        "            depth = self.to_tensor(depth).float()*10\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def to_tensor(self, pic):\n",
        "        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n",
        "            raise TypeError(\n",
        "                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n",
        "\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
        "\n",
        "            return img.float().div(255)\n",
        "\n",
        "        if accimage is not None and isinstance(pic, accimage.Image):\n",
        "            nppic = np.zeros(\n",
        "                [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
        "            pic.copyto(nppic)\n",
        "            return torch.from_numpy(nppic)\n",
        "\n",
        "        # handle PIL Image\n",
        "        if pic.mode == 'I':\n",
        "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
        "        elif pic.mode == 'I;16':\n",
        "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
        "        else:\n",
        "            img = torch.ByteTensor(\n",
        "                torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
        "        if pic.mode == 'YCbCr':\n",
        "            nchannel = 3\n",
        "        elif pic.mode == 'I;16':\n",
        "            nchannel = 1\n",
        "        else:\n",
        "            nchannel = len(pic.mode)\n",
        "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
        "        # put it from HWC to CHW format\n",
        "        # yikes, this transpose takes 80% of the loading time/CPU\n",
        "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        if isinstance(img, torch.ByteTensor):\n",
        "            return img.float().div(255)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "\n",
        "        image = self.normalize(image, self.mean, self.std)\n",
        "\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def normalize(self, tensor, mean, std):\n",
        "        \"\"\"Normalize a tensor image with mean and standard deviation.\n",
        "        See ``Normalize`` for more details.\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
        "            std (sequence): Sequence of standard deviations for R, G, B channels\n",
        "                respecitvely.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, mean, std):\n",
        "            t.sub_(m).div_(s)\n",
        "        return tensor"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fr_KMVKLDrO"
      },
      "source": [
        "__imagenet_pca = {\n",
        "        'eigval': torch.Tensor([0.2175, 0.0188, 0.0045]),\n",
        "        'eigvec': torch.Tensor([\n",
        "            [-0.5675,  0.7192,  0.4009],\n",
        "            [-0.5808, -0.0045, -0.8140],\n",
        "            [-0.5836, -0.6948,  0.4203],\n",
        "        ])\n",
        "    }\n",
        "__imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
        "                        'std': [0.229, 0.224, 0.225]}\n",
        "\n",
        "trans = transforms.Compose([\n",
        "                                           Scale([320,240]),\n",
        "                                           CenterCrop([304, 228], [304, 228]),\n",
        "                                           ToTensor(is_test=True),\n",
        "                                           Normalize(__imagenet_stats['mean'],\n",
        "                                                     __imagenet_stats['std'])\n",
        "                                       ])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slvdd2-Y1Xjd"
      },
      "source": [
        "def define_model(is_resnet, is_densenet, is_senet):\n",
        "    # if is_resnet:\n",
        "    #     original_model = resnet.resnet50(pretrained = True)\n",
        "    #     Encoder = modules.E_resnet(original_model) \n",
        "    #     model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n",
        "    # if is_densenet:\n",
        "    #     original_model = densenet.densenet161(pretrained=True)\n",
        "    #     Encoder = modules.E_densenet(original_model)\n",
        "    #     model = net.model(Encoder, num_features=2208, block_channel = [192, 384, 1056, 2208])\n",
        "    if is_senet:\n",
        "        original_model = senet154(pretrained='imagenet')\n",
        "        Encoder = E_senet(original_model)\n",
        "        model = model_enc(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv11XNNx1bDB"
      },
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "def main():\n",
        "    model = define_model(is_resnet=False, is_densenet=False, is_senet=True)\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "    model.load_state_dict(torch.load('/content/drive/My Drive/Revisiting_Single_Depth_Estimation/pretrained_model/model_senet'))\n",
        "    model.eval()\n",
        "\n",
        "    # nyu2_loader = loaddata.readNyu2('data/demo/img_nyu2.png')\n",
        "\n",
        "#####################np file\n",
        "    # image = Image.open('/content/drive/My Drive/data/0077.png').convert('RGB')\n",
        "    # image = np.load(os.path.join(\"/content/drive/My Drive/data/\", 'eigen_test_rgb.npy'))\n",
        "    # print(image[0].shape)\n",
        "    # depth = np.load(os.path.join(\"/content/drive/My Drive/data/\", 'eigen_test_depth.npy'))\n",
        "\n",
        "#####################jpg-png file\n",
        "    image = Image.open(os.path.join(\"/content/drive/My Drive/data/\", '0049.jpg'))\n",
        "    depth = Image.open(os.path.join(\"/content/drive/My Drive/data/\", '2.png'))\n",
        "\n",
        "    # image = Image.fromarray(image[0])\n",
        "    # depth = Image.fromarray(np.uint8(depth[0]))\n",
        "\n",
        "    sample = {'image': image, 'depth': depth}\n",
        "    # image = ToTensor()(image[0]).unsqueeze(0) # unsqueeze to add artificial first dimension\n",
        "    sample = trans(sample)\n",
        "    print(sample['image'].shape)\n",
        "    # image = Variable(image)\n",
        "    test(model,sample['image'].unsqueeze(0) )"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrDbJTac1dyp"
      },
      "source": [
        "def test(model,image):\n",
        "    out = model(image)\n",
        "\n",
        "# for i, image in enumerate(nyu2_loader):\n",
        "#         # image = torch.autograd.Variable(image, volatile=True).cuda()\n",
        "#         out = model(image)\n",
        "        \n",
        "    matplotlib.image.imsave('/content/drive/My Drive/data/pix3d4.png', out.view(out.size(2),out.size(3)).data.cpu().numpy())\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neF1NpYE1iqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758fa37c-7789-4fee-b313-5215a8f94490"
      },
      "source": [
        "main()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 228, 304])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUm7o4wqmzjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}