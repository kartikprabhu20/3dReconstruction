{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseDepth.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2XsAGv9UOv9"
      },
      "source": [
        "Code borrowed from : https://github.com/ialhashim/DenseDepth "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se6p5YPXTwuO",
        "outputId": "9c0cdde3-aad0-4a5c-b80d-78e71d1287b1"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "py_file_location = \"/content/drive/My Drive/DenseDepth\"\n",
        "sys.path.append(py_file_location)\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "import collections\n",
        "\n",
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9xNuuNUWUQd",
        "outputId": "b82c0e13-4fbd-409a-e908-139a614609c2"
      },
      "source": [
        "cd '/content/drive/My Drive/DenseDepth/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DenseDepth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_2wrGVmcSiz"
      },
      "source": [
        "# !wget https://s3-eu-west-1.amazonaws.com/densedepth/nyu.h5 -O /content/drive/\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2ejYFpdmxga1",
        "outputId": "e731f42d-8935-4764-c078-96a33a962c60"
      },
      "source": [
        "import torch\n",
        "import torch.nn.parallel\n",
        "\n",
        "# import senet\n",
        "# import modules\n",
        "# import resnet\n",
        "# import net\n",
        "# import densenet\n",
        "\n",
        "import matplotlib.image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap(\"jet\")\n",
        "from PIL import Image\n",
        "\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils import model_zoo\n",
        "import copy\n",
        "import numpy as np\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX8mhDvRRFrs"
      },
      "source": [
        "from keras.engine.topology import Layer, InputSpec\n",
        "import keras.utils.conv_utils as conv_utils\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "def normalize_data_format(value):\n",
        "    if value is None:\n",
        "        value = K.image_data_format()\n",
        "    data_format = value.lower()\n",
        "    if data_format not in {'channels_first', 'channels_last'}:\n",
        "        raise ValueError('The `data_format` argument must be one of '\n",
        "                         '\"channels_first\", \"channels_last\". Received: ' +\n",
        "                         str(value))\n",
        "    return data_format\n",
        "    \n",
        "\n",
        "class BilinearUpSampling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n",
        "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
        "        self.data_format = normalize_data_format(data_format)\n",
        "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n",
        "        self.input_spec = InputSpec(ndim=4)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_first':\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
        "            return (input_shape[0],\n",
        "                    input_shape[1],\n",
        "                    height,\n",
        "                    width)\n",
        "        elif self.data_format == 'channels_last':\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
        "            return (input_shape[0],\n",
        "                    height,\n",
        "                    width,\n",
        "                    input_shape[3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = K.shape(inputs)\n",
        "        if self.data_format == 'channels_first':\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
        "        elif self.data_format == 'channels_last':\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
        "        \n",
        "        return tf.image.resize(inputs, [height, width], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'size': self.size, 'data_format': self.data_format}\n",
        "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBVynY3rPNIx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class UpSample(nn.Sequential):\n",
        "    def __init__(self, skip_input, output_features):\n",
        "        super(UpSample, self).__init__()        \n",
        "        self.convA = nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1)\n",
        "        self.leakyreluA = nn.LeakyReLU(0.2)\n",
        "        self.convB = nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1)\n",
        "        self.leakyreluB = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, concat_with):\n",
        "        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n",
        "        return self.leakyreluB( self.convB( self.convA( torch.cat([up_x, concat_with], dim=1)  ) )  )\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_features=1664, decoder_width = 1.0):\n",
        "        super(Decoder, self).__init__()\n",
        "        features = int(num_features * decoder_width)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_features, features, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.up1 = UpSample(skip_input=features//1 + 256, output_features=features//2)\n",
        "        self.up2 = UpSample(skip_input=features//2 + 128,  output_features=features//4)\n",
        "        self.up3 = UpSample(skip_input=features//4 + 64,  output_features=features//8)\n",
        "        self.up4 = UpSample(skip_input=features//8 + 64,  output_features=features//16)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(features//16, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x_block0, x_block1, x_block2, x_block3, x_block4 = features[3], features[4], features[6], features[8], features[12]\n",
        "        x_d0 = self.conv2(F.relu(x_block4))\n",
        "\n",
        "        x_d1 = self.up1(x_d0, x_block3)\n",
        "        x_d2 = self.up2(x_d1, x_block2)\n",
        "        x_d3 = self.up3(x_d2, x_block1)\n",
        "        x_d4 = self.up4(x_d3, x_block0)\n",
        "        return self.conv3(x_d4)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()       \n",
        "        self.original_model = models.densenet169( pretrained=False )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for k, v in self.original_model.features._modules.items(): features.append( v(features[-1]) )\n",
        "        return features\n",
        "\n",
        "class PTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PTModel, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder( self.encoder(x) )\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54NBaNIzlyx1"
      },
      "source": [
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
        "    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
        "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    \"\"\"\n",
        "    def __init__(self,is_test=False):\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
        "        Returns:\n",
        "            Tensor: Converted image.\n",
        "        \"\"\"\n",
        "        # ground truth depth of training samples is stored in 8-bit while test samples are saved in 16 bit\n",
        "        image = self.to_tensor(image)\n",
        "        if self.is_test:\n",
        "            depth = self.to_tensor(depth).float()/1000\n",
        "        else:            \n",
        "            depth = self.to_tensor(depth).float()*10\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def to_tensor(self, pic):\n",
        "        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n",
        "            raise TypeError(\n",
        "                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n",
        "\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
        "\n",
        "            return img.float().div(255)\n",
        "\n",
        "        if accimage is not None and isinstance(pic, accimage.Image):\n",
        "            nppic = np.zeros(\n",
        "                [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
        "            pic.copyto(nppic)\n",
        "            return torch.from_numpy(nppic)\n",
        "\n",
        "        # handle PIL Image\n",
        "        if pic.mode == 'I':\n",
        "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
        "        elif pic.mode == 'I;16':\n",
        "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
        "        else:\n",
        "            img = torch.ByteTensor(\n",
        "                torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
        "        if pic.mode == 'YCbCr':\n",
        "            nchannel = 3\n",
        "        elif pic.mode == 'I;16':\n",
        "            nchannel = 1\n",
        "        else:\n",
        "            nchannel = len(pic.mode)\n",
        "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
        "        # put it from HWC to CHW format\n",
        "        # yikes, this transpose takes 80% of the loading time/CPU\n",
        "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        if isinstance(img, torch.ByteTensor):\n",
        "            return img.float().div(255)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "\n",
        "        image = self.normalize(image, self.mean, self.std)\n",
        "\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def normalize(self, tensor, mean, std):\n",
        "        \"\"\"Normalize a tensor image with mean and standard deviation.\n",
        "        See ``Normalize`` for more details.\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
        "            std (sequence): Sequence of standard deviations for R, G, B channels\n",
        "                respecitvely.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: make efficient\n",
        "        for t, m, s in zip(tensor, mean, std):\n",
        "            t.sub_(m).div_(s)\n",
        "        return tensor"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4QViWExQN7y"
      },
      "source": [
        "def DepthNorm(x, maxDepth):\n",
        "    return maxDepth / x\n",
        "\n",
        "def predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n",
        "    # Support multiple RGBs, one RGB image, even grayscale \n",
        "    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n",
        "    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n",
        "    # Compute predictions\n",
        "    predictions = model.predict(images, batch_size=batch_size)\n",
        "    # Put in expected range\n",
        "    return np.clip(DepthNorm(predictions, maxDepth=maxDepth), minDepth, maxDepth) / maxDepth\n",
        "\n",
        "# def load_model():\n",
        "#     # Kerasa / TensorFlow\n",
        "#     os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
        "#     from keras.models import load_model\n",
        "\n",
        "#     # Custom object needed for inference and training\n",
        "#     custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}\n",
        "\n",
        "#     # Load model into GPU / CPU\n",
        "#     return load_model('nyu.h5', custom_objects=custom_objects, compile=False)\n",
        "\n",
        "\n",
        "def load_test_data(test_data_zip_file='nyu_test.zip'):\n",
        "    print('Loading test data...', end='')\n",
        "    import numpy as np\n",
        "    from data import extract_zip\n",
        "    data = extract_zip(test_data_zip_file)\n",
        "    from io import BytesIO\n",
        "    rgb = np.load(BytesIO(data['eigen_test_rgb.npy']))\n",
        "    depth = np.load(BytesIO(data['eigen_test_depth.npy']))\n",
        "    # crop = np.load(BytesIO(data['eigen_test_crop.npy']))\n",
        "\n",
        "def test(model,image):\n",
        "    out = model(image)\n",
        "    # predict_test = cmap(predict(model, image, minDepth=minDepth, maxDepth=maxDepth)[0,:,:,0])[:,:,:3]\n",
        "    print(out.shape)\n",
        "    matplotlib.image.imsave('/content/drive/My Drive/data/test.png', out)\n",
        "\n",
        "\n",
        "\n",
        "def my_predict(model, images, minDepth=10, maxDepth=1000):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Compute predictions\n",
        "    predictions = model(images.float())\n",
        "\n",
        "  # return predictions\n",
        "    # Put in expected range\n",
        "  return np.clip(DepthNorm(predictions.numpy(), maxDepth=maxDepth), minDepth, maxDepth) / maxDepth\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ps1YGqvOp2e"
      },
      "source": [
        "import os \n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_model_pytorch():\n",
        "  print('Loading model...')\n",
        "\n",
        "  custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}\n",
        "\n",
        "  # Load model into GPU / CPU\n",
        "  model = load_model('nyu.h5', custom_objects=custom_objects, compile=False)\n",
        "  names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "  weights = model.get_weights()\n",
        "\n",
        "  keras_name = []\n",
        "  for name, weight in zip(names, weights):\n",
        "    keras_name.append(name)\n",
        "\n",
        "  pytorch_model = PTModel().float()\n",
        "\n",
        "  # load parameter from keras\n",
        "  keras_state_dict = {} \n",
        "  j = 0\n",
        "  for name, param in pytorch_model.named_parameters():\n",
        "  \n",
        "    if 'classifier' in name:\n",
        "      keras_state_dict[name]=param\n",
        "      continue\n",
        "\n",
        "    if 'conv' in name and 'weight' in name:\n",
        "      keras_state_dict[name]=torch.from_numpy(np.transpose(weights[j],(3, 2, 0, 1)))\n",
        "      # print(name,keras_name[j])\n",
        "      j = j+1\n",
        "      continue\n",
        "  \n",
        "    if 'conv' in name and 'bias' in name:\n",
        "      keras_state_dict[name]=torch.from_numpy(weights[j])\n",
        "      # print(param.shape,weights[j].size)\n",
        "      j = j+1\n",
        "      continue\n",
        "\n",
        "    if 'norm' in name and 'weight' in name:\n",
        "      keras_state_dict[name]=torch.from_numpy(weights[j])\n",
        "      # print(param.shape,weights[j].shape)\n",
        "      j = j+1\n",
        "      continue\n",
        "\n",
        "    if 'norm' in name and 'bias' in name:\n",
        "      keras_state_dict[name]=torch.from_numpy(weights[j])\n",
        "      # print(param.shape,weights[j].size)\n",
        "      j = j+1\n",
        "      keras_state_dict[name.replace(\"bias\", \"running_mean\")]=torch.from_numpy(weights[j])\n",
        "      # print(param.shape,weights[j].size)\n",
        "      j = j+1\n",
        "      keras_state_dict[name.replace(\"bias\", \"running_var\")]=torch.from_numpy(weights[j])\n",
        "      # print(param.shape,weights[j].size)\n",
        "      j = j+1\n",
        "      continue\n",
        "\n",
        "\n",
        "  pytorch_model.load_state_dict(keras_state_dict)\n",
        "  pytorch_model.eval()\n",
        "  return pytorch_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Riqn-hls1n"
      },
      "source": [
        "__imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
        "                        'std': [0.229, 0.224, 0.225]}\n",
        "\n",
        "trans = transforms.Compose([ToTensor(is_test=True),\n",
        "                                           Normalize(__imagenet_stats['mean'],\n",
        "                                                     __imagenet_stats['std'])\n",
        "                                       ])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgP18rHRftB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069e449b-bdbf-4a2d-d30d-db87ebb34d4c"
      },
      "source": [
        "pytorch_model = load_model_pytorch()\n",
        "cmap = plt.get_cmap('jet')\n",
        "minDepth, maxDepth = 10, 1000\n",
        "\n",
        "inputs = np.clip(np.asarray(Image.open(os.path.join(\"/content/drive/My Drive/data/\", 'test_img.png')))/255,0,1)\n",
        "# x = Image.open(os.path.join(\"/content/drive/My Drive/data/\", '0049.jpg'))\n",
        "# x = trans(x)\n",
        "# x = torch.tensor(x)\n",
        "pytorch_input = torch.from_numpy(inputs).permute(2,0,1).unsqueeze(0)\n",
        "print(pytorch_input.shape)\n",
        "# x = np.expand_dims(x, axis=0)\n",
        "out = my_predict(pytorch_model,pytorch_input[0,:,:,:].unsqueeze(0))[0]\n",
        "print(out.shape)\n",
        "# plt.imshow(out[0,:,:])\n",
        "# plt.savefig('test.png')\n",
        "# plt.show()\n",
        "matplotlib.image.imsave('/content/drive/My Drive/data/synthetic1.png', out[0,:,:])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "torch.Size([1, 3, 820, 1078])\n",
            "(1, 410, 539)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYvLQ4OPQbsy"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}