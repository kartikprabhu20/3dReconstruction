\chapter{\iftoggle{german}{Konzept}{Concept}}\label{ch:concept}

In this chapter, we discuss the critical datasets used to create a new synthetic dataset and the reason for their selection in \autoref{sec:3d-furniture-models-from-pix3d}.
Furthermore, how we combine multiple datasets to our advantage of creating new datasets in the Ersatz environment.
In \autoref{sec:s2r:3d-free-a-pix3d-based-synthetic-dataset}, we will introduce the proposed \gls{free} dataset used for the 3D-reconstruction task.

The rationale for choosing Unity as a framework to build the pipeline will be discussed in \autoref{sec:unity-based-pipeline}.
In \autoref{sec:3d-scene-framework}, we introduce a new tool to create a synthetic dataset, and discuss in detail the design of the application and the rationale behind each module.

In the Deep Learning domain(\autoref{subsec:why-pix3d?}), we explain the rationale behind selecting baseline models for the 3D-reconstruction task.

In \autoref{sec:datasets}, we describe various synthetic datasets created using the 3D-Scene tool for 3D-Reconstruction tasks.
The collection will also contain chair datasets that will be used to study the effect of domain randomization.

\section{A New Dataset for 3D Reconstruction Task}\label{sec:3d-furniture-models-from-pix3d}
As discussed in \autoref{subsec:indoor-synthetic-datasets}, we have several synthetic datasets.
However, not many support 3D reconstruction tasks.
The dataset, which supports 3D reconstruction, does not have real-world images to compare the results.
This section discusses the datasets we considered for creating a new synthetic dataset and the reason for the selection.

\subsection{Disadvantages of Pix3D}\label{subsec:disadvantages-of-pix3d}
The distribution of models in Pix3D is as shown in \autoref{fig:pix3d_histogram}.
As we can see, the dataset distribution is uneven across classes, and more than 50\% of classes have less than 1000 images.

Though Pix3D set a benchmark for 2D-3D alignment, here are few disadvantages of using this real dataset.
\begin{enumerate}
    \item For Deep Learning approaches, we need large-scale data, and 14,600 images for Nine categories might not be sufficient.
    \item The orientation of an object is not randomized.
    \item The dataset does not provide 2.5D information (i.e., depth maps and normals), which can be crucial for 3D learning.
\end{enumerate}

\begin{figure}[!ht]
    \resizebox{0.49\textwidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/pix3d/pix3d_histogram.pgf}}
    \resizebox{0.49\textwidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/pix3d/pix3d_models_histogram.pgf}}
    \caption[Distribution of Pix3D.]{Distribution of Pix3D~\cite{Sun2018} images(left), unique models(right)}
    \label{fig:pix3d_histogram}
\end{figure}

\subsection{Why Pix3D?}\label{subsec:why-pix3d?}
We discuss the drawbacks of Pix3D in \autoref{subsec:disadvantages-of-pix3d}.
Apart from those reasons, Pix3D is a collection of indoor scenes with a complex background, varying light conditions with shadows, reflective surfaces, and even varying occlusion levels.
Each image comprises a collection of objects in the scene, but only one object from the category is annotated.
It is a perfect example of having limited real-world data.
Since 3D models are available for each piece of furniture, synthetic data can be generated in abundance from those models.
It would be interesting to see if a Game Engine can replicate such complex features of the real world or create randomization to an extent where such complex images appear to be a part of randomization.
A study on Pix3D and synthetic datasetsThe placement is done such that the contex should demonstrate the usefulness of Game Engines.
For our synthetic to real dataset experiment, we chose to select only furniture classes from Pix3D, leaving out ``misc and tools" classes, which were significantly less in comparison.

\subsection{Role of SceneNet}\label{sec:role-of-scenenet}
Now that we decided on the 3D furniture models, we need an environment with complex background.
SceneNet~\cite{McCormac2017} is an extensive collection of photorealistic indoor scene trajectories.
In our approach, we utilize the scene provided by SceneNet as a layout for our indoor scenes.
Each scene includes initial Shapenet models with furniture placement.
The placement is done such that the context between the the pieces of furniture is maintained.
For example, chairs are placed near the table,
Sofa and coffee table are facing towards a television, etc.
Using these position, we will be able to derive sensible images for our synthetic dataset.

\subsection{\gls{free}, a Pix3D Based Synthetic Dataset}\label{sec:s2r:3d-free-a-pix3d-based-synthetic-dataset}

\gls{free} dataset, which stands for Synth2Real: 3-Dimensional Furniture Reconstruction from Ersatz Environment, combines SceneNet and Pix3D dataset.
We utilize the availability of 3D models of rooms and pieces of furniture from these two datasets to create an ersatz environment using Unity as our framework.
We randomize the indoor scenes from SceneNet~\cite{McCormac2017} and textures provided by them with some additional complex textures.
The ShapeNet and Pix3D datasets have common categories.
We replace 3D furniture in the scene with a model from Pix3D of the same type.
This process is randomized and will be discussed in \autoref{sec:3d-scene-framework}.
The other option would have been to use the ShapeNet or \gls{front} as the target model, but we would not have a real dataset to compare to this case.
Ideally, a model trained on ShapeNet should also perform well with a real dataset like Pix3D, but we decided to have a synthetic dataset based on Pix3D itself for better comparison.

\section{Unity-Based Pipeline}\label{sec:unity-based-pipeline}
To create the synthetic dataset, we use Unity Game Engine.
The reasons for selecting Unity as our platform for the application are:
\begin{enumerate}
    \item Cross-platform game engine and hence usable on any Operating system.
    \item The basic version is available for free.
    \item Provides all the necessary tools to create an ersatz environment with well-maintained documentation.
    \item An active developer community.
\end{enumerate}

There is no official comparison between Unreal Engine and Unity engine to have a deciding factor.
The selection of the Unity engine was a purely individual choice and ease of use.
However, both these game engines can create realistic-looking scenes that are usable for synthetic dataset generation.
With the Unity game engine, the available scenes from the Scenenet and 3D models from Pix3D can be imported to form an ersatz environment.
Further domain randomization can be applied to create a dataset of photorealistic images and 2.5D data like normals, depth maps, masks.


\section{3D-Scene Framework}\label{sec:3d-scene-framework}
3D-Scene is a Unity-based application created as a proof of concept for generating photorealistic images using a Game engine.
The core pipeline and the workflow is as shown in \autoref{fig:pipeline process}.
For an automation-based pipeline, the process loops through each step, while in the manual pipeline, the user decides which block to execute.
In this section, each of the blocks is explained briefly.
The implementation of the blocks will be discussed in \autoref{ch:implementation}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/concept/process}
    \caption[Overview of 3D-Scene Tool]{An overview of pipeline for image generation with different blocks and external libraries.
    Each block is discussed in sections mentioned in the figure.}
    \label{fig:pipeline process}
\end{figure}

\subsection{Furniture Models from Pix3D}\label{subsec:furniture-models-from-pix3d}
As discussed in \autoref{subsec:why-pix3d?}, we utilize 3D furniture models from Pix3D.
Pix3D provides 381 models for pieces of furniture with seven categories(Bed, Bookcase, Chair, Desk, Sofa, Table, Wardrobe).
Most of the models have default textures, while some do not.
These textures will be changed randomly within the pipeline.

\subsection{Scenes from SceneNet}\label{subsec:scenes-from-scenenet}
We utilize scenes provided by SceneNet~\cite{McCormac2017} for the rooms in our ersatz environments.
All the categories from Pix3D are also present in ShapeNet, which are used to fill the space in SceneNet.
The distribution of furniture matching the category of Pix3D in the scenes is as shown in \autoref{fig:distribution of scenes}.
The application modifies each scene for every snap we take for the dataset we create at random.

\begin{figure}[!ht]
    \resizebox{0.49\textwidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/types_of_scenes.pgf}}
    \resizebox{0.49\textwidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/distribution_categories_scenenet.pgf}}
    \caption[Distribution of SceneNet.]{(Left)Distribution of types of scenes, (Right) Distribution of objects matching the categories of Pix3D in scenes.
    The chair category has most number of possible positions, while wardrobe has the least.}
    \label{fig:distribution of scenes}
\end{figure}

SceneNet has made 25 rooms available to the public.
We only use scenes of type Bedroom(11), Kitchen(1), Living room(6), and office(7).
We did not consider the bathroom for our case, as the categories under observations are furniture which we rarely see in the bathroom
\autoref{fig:Scene Types} shows different types of scenes utilized to generate a synthetic dataset.
These scenes are further modified by adding a few more objects of categories present in Pix3D to get more variations in the dataset.


\begin{figure}[!ht]
    \centering
    \subfloat[][]{\includegraphics[width=.35\textwidth, height = .35\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/scene_bedroom}}\quad
    \subfloat[][]{\includegraphics[width=.35\textwidth, height = .35\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/scene_livingroom}}\\
    \subfloat[][]{\includegraphics[width=.35\textwidth, height = .35\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/scene_kitchen}}\quad
    \subfloat[][]{\includegraphics[width=.35\textwidth, height = .35\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/scene_office}}
    \caption[Top View for SceneNet Layouts]{The top view of sample scene layouts from SceneNet. Types: (a)Bedroom, (b)LivingRoom, (c)Kitchen and (d)Office.}
    \label{fig:Scene Types}
\end{figure}

\subsection{Randomized Texture}\label{subsec:randomised-texture}

SceneNet~\cite{McCormac2017} also provide textures for different categories in the scene.
We further increase the texture database by adding more textures from ambientCG.com, which provides free licenses.
A total of 982 texture images with 58 regular object categories, some of which are just JPG or PNG images, while there are few textures from cgtextures\footnote{https://www.textures.com/} with more details like normals, displacement, and roughness.
This makes the textures more realistic.
The distribution of the top 40 texture categories used for randomization of scenes is as shown in \autoref{fig:Distribution of textures}, with categories of Pix3D
having a higher number of images.
Each scene is randomized for every snap taken of the target object.
Samples of texture randomization of background in the scene with the target object in focus are shown in \autoref{fig:Texture Randomisation}.

The textures need to be grouped together into a folder with category names.
Since this library is external to Unity, the users can add or remove textures according to their preferences.
Even new categories can be added by just creating a new folder.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/background_texture1}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/background_texture2}\\
    \vspace{0.1cm}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/background_texture3}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/background_texture4}\\
    \caption[Samples for Different Textures.]{Sample images with different textures for same scene.}
    \label{fig:Texture Randomisation}
\end{figure}

\begin{figure}[!ht]
    \centering
    \resizebox{\textwidth}{11.5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/scenenet_scenes/distribution_of_textures.pgf}}
    \caption[Distribution of Textures.]{Distribution of textures used on scenes. The categories of Pix3D(target furniture) have higher number of images.}
    \label{fig:Distribution of textures}
\end{figure}

Randomizing the outdoor environment is important in cases where we have open doors and windows.
Unity provides a wrapper for scenes called \emph{Skyboxes}.
This is spherical texture around the room under observation.
This gives us a varying outdoor environment for the scene with the scenery at the horizon.
We randomize ten open license skyboxes\footnote{https://polyhaven.com/hdris/outdoor} to achieve this.
Samples of the skybox are shown in \autoref{fig:skybox samples}.


\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/skybox_1}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/skybox_2} \\
    \vspace{0.1cm}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/skybox_3}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/skybox_4}\\
    \caption[Samples for Skyboxes.]{Samples for different skyboxes which change the outdoor environment for the scenes. In the figure we see an open window with changing skybox.}
    \label{fig:skybox samples}
\end{figure}



\subsection{Replacing Target Objects}\label{subsec:replacing-target-objects}

To further randomize the scene, the category of target objects is replaced by the object under observation.
When more than one object of the same category is present, we randomize the object to be replaced, further randomizing the captured data.
The target object is scaled such that the least dimension of the target object matches the least dimension of the category object in the scene.
For example, if the length of the category object in the scene is the most petite amount length, width, and height, then the target object is scaled to match this length.
The rescaling makes the target object blend in with the scene.
Samples for replacing a target object from an original scene from Scenenet are shown in \autoref{fig:replace model}.

\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/replace_1-1}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/replace_1-2} \\
    \vspace{0.1cm}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/replace_2-1}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/replace_2-2}\\
    \caption[Samples for Object Replacement]{Samples for object replacement. The Left column shows a scene from SceneNet, while the right column shows an object being replaced in original scene.}
    \label{fig:replace model}
\end{figure}


\subsection{Camera ViewPoints}\label{subsec:camera-viewpoints}

We randomize camera position with some constraints such that we get different orientations of the target object with different backgrounds.
The constraints will include the height of the camera, minimum and maximum distance to the target object.
A random point is selected within this bound for the camera position.
We make sure that the target object is within the camera frame and is visible.
\autoref{fig:Camera viewpoints} shows samples of different camera viewpoints on a constant object and scene.

\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/camera1}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/camera2}\\
    \vspace{0.1cm}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/camera3}
    \includegraphics[width=.4\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/camera4}\\
    \caption[Samples for Camera ViewPoints.]{Sample images with different camera viewpoints of same object with a constant scene.}
    \label{fig:Camera viewpoints}
\end{figure}

\subsection{Lightings and Shadows}\label{subsec:lightings-and-shadows}

Lighting plays a vital role in photorealism.
The shadows formed with different lighting conditions enhance the photorealism of the images.
Unity offers a wide variety of illumination like global light, which acts like sunlight, and various indoor lighting systems.
Ideally, we should make the luminous objects like lamps, chandeliers, bulbs, etc.,
the source of light for indoor scenes, but we observed that the room does not light up uniformly, making it less photorealistic.
Hence we use some pre-determined lighting settings, discussed further in implementation(Section 4.1).

%\begin{figure}
%    \centering
%    \includegraphics[width=.3\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting1}
%    \includegraphics[width=.3\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting2}\\
%    \vspace{0.1cm}
%    \includegraphics[width=.3\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting3}
%    \includegraphics[width=.3\textwidth, height = .3\textwidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting4}\\
%    \caption{Sample images with different lighting and shadows conditions}
%    \label{fig:Lighting and shadows}
%\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting1}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting2}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting3}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting4}\\
    \vspace{0.1cm}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting5}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting6}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting7}
    \includegraphics[width=.24\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/implementation/randomisation/lighting8}\\
    \caption[Samples for Lightings and Shadows.]{Sample images with different lighting and shadows conditions.First row is samples for light with different intensity and direction. Second row is differnt color for light.}
    \label{fig:Lighting and shadows}
\end{figure}

\section{3D Reconstruction Pipeline: Why Pix2Vox?}\label{sec:3D reconstruction pipeline}
We create a Deep Learning pipeline for processing the 3D reconstruction task.
The backbone of the pipeline is the base model and the dataset being used to train.
In this section, we discuss the model used as a base and the rationale behind its selection.

Pix2Vox has been used as a baseline by most of the research-oriented to 3D reconstruction.
This network is one of the few networks to be tested on the Pix3D dataset.
According to the survey conducted by~\cite{Han2021ImageBased3O}, the performance of Pix2Vox~\cite{Xie_2019}
is significantly higher compared to previous work(\cite{Tulsiani2017,tatarchenko2016multiview,Roth2018,Gwak2018,Johnston2017}), as shown in \autoref{fig:survey on 3d reconstruction}.
At the same time, this comparison was made on the 3D reconstruction of the ShapeNet dataset since Pix3D was not available when previous work was published.
From our survey, only CoReNet~\cite{popov2020corenet} had a slight gain in performance compared to Pix2Vox.
When trained on ShapeNet and tested with Pix3D, CoReNet gave a result of 29.7\% \gls{iou} while Pix2Vox gave a result of 28.8\% \gls{iou}  and Pix2Vox++ a result of 29.2\% \gls{iou}\@.
Since the difference in the performance was not significant, we decided to stick with the baseline model.
Another reason for selecting the Pix2Vox model is that the backbone of the architecture is pre-trained with ImageNet.
Hence, the embeddings generated from this encoder can help visualize the domain space of both Pix3D (real images)  and \gls{free}(synthetic images).
As mentioned above, for Pix2Vox++, the \gls{resnet} is the backbone encoder with 25\% lesser parameters and 5\% lesser inference time than \gls{vgg}\@.
In addition, the author even demonstrated that Pix2vox++ performs 1.5\% better than Pix2Vox.
The architecture of Pix2Vox++ is as shown in \autoref{fig:architectures}(b).
Furthermore, the focus of this thesis is not to check which is the best model to reconstruct the furniture but to check if game engines can produce photorealistic images usable for 3D reconstruction.
Hence the selection of the model was not of utmost importance.
However, since the two architectures are relatable, it would be interesting to compare the results for the 3D Reconstruction task.

%\begin{figure}
%    \centering
%    \includegraphics[width=\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/concept/pix2vox}
%    \caption{Network architecture for pix2vox~\cite{Xie_2019}}
%    \label{fig:pix2vox architecture}
%\end{figure}
%
%\begin{figure}
%    \centering
%    \includegraphics[width=\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/concept/pix2voxpp}
%    \caption{Network architecture for pix2vox++~\cite{Xie_2020}}
%    \label{fig:pix2voxpp architecture}
%\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/concept/survey}
    \caption[Survey Results for 3D-Reconstruction.]{A survey conducted by~\cite{Han2021ImageBased3O}, demonstrates that Pix2Vox is considerably a good 3D reconstruction model.
    The values are from 3D reconstruction of ShapeNet~\cite{shapenet2015} since Pix3D was not published by then.}
    \label{fig:survey on 3d reconstruction}
\end{figure}


\section{Datasets}\label{sec:datasets}
In this section, we describe the datasets used for the evaluations.
Datasets intend to have variations in domain randomization to check their performance on the 3D reconstruction tasks.

\subsection{Pix3D}\label{subsec:pix3d}
As mentioned in \autoref{subsec:why-pix3d?}, we use a real dataset from~\cite{Sun2018}, a collection of indoor scenes.
The two classes ’misc’ and ’tools’ are eliminated to focus only on furniture.
The total images after the reduction are 9954 with 354 unique models.
The dataset is divided into 70:30, giving us 6814 images from training and 3140 images for validation/test.
We do not have a test set only for this dataset since it is already limited, and the validation set is used as a test set while testing with synthetic data.
Samples are as shown in \autoref{fig:samples for synthetic and real comparison}.

\subsection{\gls{s2rv1}}\label{subsec:gls{free}-version-1}
Version 1 of \gls{free} was created by keeping the models in the center of a default 3D room.
The camera distance was randomized between 1 and 2.5 meters from the model.
The camera viewpoints and textures were randomized.
A total of 70000 images were synthetically generated using the \gls{free} `Single Room pipeline' with 10000 images per category.
Samples are as shown in \autoref{fig:samples for synthetic and real comparison}.

\subsection{\gls{s2rv2}}\label{subsec:gls{free}-version-2}
Version 1 of \gls{free} was created by keeping the models in the center of a default 3D room.
The distance of the camera was randomly chosen between 1 and 2.5 meters from the model.
The camera viewpoints and textures were randomized.
A total of 21000 images were synthetically generated using the \gls{free} `Multi-Object pipeline' with 3000 images per category.
Samples are as shown in \autoref{fig:samples for synthetic and real comparison}.

\begin{figure}[ht]
    \centering
    \begin{tabular}{llll}
        Pix3D & \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_1} &
        \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_2} &
        \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_3}\\

        \gls{s2rv1} & \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_1} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_2} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_3}\\

        \gls{s2rv2} & \includegraphics[width=.19\textwidth, height =.2\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_1} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_2} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_3}\\

    \end{tabular}
    \caption[Samples for Real and Synthetic Datasets]{Samples of images from real and synthetic datasets.}
    \label{fig:samples for synthetic and real comparison}
\end{figure}

\subsection{\gls{free} Ablation}\label{subsec:s2r:3dfree-ablation}
Pix3D is composed of 3839 chairs which is the maximum among all categories.
To study the effects of domain randomization, we create multiple chair datasets by omitting randomizing factors one at a time and study the model behavior.
We divide the datasets into five categories with different randomization parameters.
The sample images with different randomization are as shown in \autoref{fig:domain_randomisation_for_ablation_study}.

\begin{figure}[!ht]
    \begin{tabular}{llll}
        Textureless & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_3}\\

        Textureless with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_3}\\

        Textured & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_3}\\

        Textured with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_4}\\

        Multi-Object & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_3}\\

    \end{tabular}
    \caption[Samples for Datasets Used for Ablastion Study.]{Samples of images used for ablation study on chairs with different parameters of domain randomization.}
    \label{fig:domain_randomisation_for_ablation_study}
\end{figure}

%\begin{figure}
%    \centering
%    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_chair_DR.pgf}}
%    \caption{A combined \gls{tsne} visualization for chair images from Pix3d and \gls{free} dataset with different parameters for randomization.}
%    \label{fig:pix3dchair_s2r3dfreechair}
%\end{figure}

\subsubsection{\gls{free}\_Textureless}

The chair models were kept at the center of an un-textured room with a constant light source for the textureless dataset.
A total of 10000 images were generated from different camera viewpoints.

\subsubsection{\gls{free}\_Textureless\_Light}

Like the textureless dataset, the chair models were kept at the center of an un-textured room for this dataset.
However, the light source was randomized.
The light variation was implemented as in \autoref{subsec:lightings-and-shadows}.
Along with randomized light sources, the camera viewpoints were randomized with a distance in the range of 0.75 to 1.5 meters from the model under observation.

\subsubsection{\gls{free}\_Textured}

As the name suggests, both the model and the single default room were textured randomly for each snapshot, as explained in \autoref{subsec:randomised-texture}.
Ten thousand snapshots of chair models were taken using different camera viewpoints.

\subsubsection{\gls{free}\_Textured\_Light}

This dataset is an extension of the above mention \gls{free}\_Textured dataset, with randomized light sources.
The lights are randomized as implemented in \autoref{subsec:lightings-and-shadows}.

\subsubsection{\gls{free}\_Multi-Object}

\gls{free}\_Multi-Object dataset was created using 'Multi-Object pipeline' with chair replacing a similar category from the scene as implemented in \autoref{subsec:replacing-target-objects}.
Both the light and camera viewpoints were randomized, ensuring that the model under observation is not completely occluded.

\subsubsection{\gls{free}\_Combined}
\gls{free}\_Combined dataset is a combination of all the five types of datasets mentioned above.
Adding 10,000 images from each dataset, we get a total of 50,000 chair images in this bundle.
This dataset is an aggregate of all types of domain randomization.





