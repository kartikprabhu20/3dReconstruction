\chapter{\iftoggle{german}{Evaluierung}{Evaluation}}\label{ch:evaluation}

\todo{
    \begin{enumerate}
        \item tsne for survey datasets
        \item tsne chair dataset with randomisation
        \item baseline comparison, pix3d pix2vox, pix2vox++(modelwise, classwise)
        \item values per category
        \item baseline different version of dataset with different threshold
        \item Finetuning over pretrained
        \item ablation values per model
        \item !!!!!finetuning with different datasize (pending)
        \item !!!!!mixed training with different real datasize(pending)
        \item output diagrams
        \item training graphs
    \end{enumerate}
}
\section{A survey on photorealism}\label{sec:a-survey-on-photorealism}
The participants were given minimalistic information about the intention behind the survey.
The goal of the survey was to analyse if humans have the same perception for photographic and computer generated images.
No time limit was set for the survey and it was open to everyone.
A total of 72 participants responded to the survey.
The survey was created using Google forms and the link was distributed.
The participants either used a mobile phone or a desktop to respond to the survey.
A total of 9 datasets were used in the survey.
3DFRONT ~\cite{Fu20203DFRONT3F}, Hypersim ~\cite{Roberts2020HypersimAP}, InteriorNet ~\cite{InteriorNet18}, SceneNet ~\cite{McCormac:etal:ICCV2017}, BlenderProc~\cite{denninger2019blenderproc},
AI2THOR ~\cite{kolve2019ai2thor}, Openroom ~\cite{li2021openrooms}, Pix3D ~\cite{pix3d} and proposed S2R:3DFREE dataset.
Only Pix3D was a real dataset while all others are synthetic dataset proclaimed to be photorealistic.

The survey was composed of 3 sections.
\begin{enumerate}
    \item Section 1: Decide if the image is real or not real.

    In this section there were a total of 27 images, 3 each from the above mentioned datasets.
    Each image had only 2 options to select: "Real" or "Not real"
    This approach eliminated any ambiguous perception towards the images.

    \item Section 2: Rate the image on scale of 1 to 10 in terms of realism (1 -> least real, 10 -> most real).

    In this section, the participant used a likert scale to rate the images based on photorealism.
    Similar to section 1, there were 27 questions of 3 images per dataset.

    \item Section 3: Rank the images from 1 to 9 (1 -> Most real, 9 -> Least real).

    In this section, the participant had only 3 questions, with each question having an image from each of the dataset arranged in a 3\x3 grid format.
    The users were asked to rank them in the increasing order of the photorealism.
\end{enumerate}

\subsection{Survey results}\label{subsec:survey-results}
In this segment, we discuss the results of survey collected from participants.

\subsubsection{Section 1: Real or Not}
In section 1, the participants had only 2 options to select from independently.
Figure ~\ref{fig:question1}, shows that the real dataset Pix3D ~\cite{pix3d} was rightly recognised as real.
77\% of the real images that belonged to Pix3D were recognised to be real.
This shows that the participants were not convinced even with the real images as 23\% of the images were still recognized are not real.
Among the synthetic datasets, Hyeperism ~\cite{Roberts2020HypersimAP} got the best results of 59.7\% identified as real.
AI2THOR had the least amount of images recognised as real with just 5\% positive responses.
The proposed S2R:3DFREE dataset had 8\% of images identified as real, which shows that images generated using the automated Unity framework needs some improvement.
Suppose we have a threshold of 50\%, we see that the datasets for which the images selected as Not real below the threshold value belong to datasets which are automated and not created by professionals manually.
As mentioned in ~\ref{subsec:indoor-synthetic-datasets}, Openrooms, SceneNet are Blenderproc are datasets obtained from automation.
We consider S2R:3DFREE dataset to be automated and belog to this category.
Among the automated images, Openrooms have got most vote of confidence with 37\% recognised as real images.
Even though S2R:3DFREE was least recognised as real among the automated tools, it had better percetage than AI2THOR which has Unity based frameworm to generate images and was manually configured by professional by taking in reference of real world images.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question1.pgf}}
    \caption{The figure represents distribution for Section 1 of survey. The participants were asked to distinguish if the image was 'Real' or 'Nor Real'.}
    \label{fig:question1}
\end{figure}

\subsubsection{Section 2: Likert Scale}
In section 2, the participants could select ratings from 1 to 10 (1 being the least photorealistic).
The distribution of values for each dataset can be seen in figure ~\ref{fig:question2} and the average ratings are as seen in figure ~\ref{fig:question2_2}.
If we consider the scale 1, which is least rating that can be given to the image, AI2THOR has most number of votes.
Suppose we have a cut off at scale 2 and 3, Openrooms and S2R:3DFREE are the least photorealistic respectively.
Interestingly Interiornet has least number of scale 1 instead of Pix3D which is the real dataset.
However, Pix3D has the highest number of perfect score(10) among all the datasets.
Coming to the averages, we again see the datasets created from automated pipeline (Blenderproc, SceneNet, Openrooms,S2R:3DFREE), have least average ratings,
while the manually created datasets(Hyperism, 3DFRONT, InteriorNet) have higher average ratings.
Pix3D has highest of the average ratings closely followed by InteriorNet.
Even though S2R:3DFREE has the least average, it is still comparable to other automated pipelines and even Unity based AI2THOR dataset.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question2.pgf}}
    \caption{The figure represents distribution for Section 2 of survey. The participants were asked to rate the image based on photorealism(1 being the least photorealistic).}
    \label{fig:question2}
\end{figure}

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question2_2.pgf}}
    \caption{The figure represents average rating given by the participant to each of the datasets in section 2 of the survey.}
    \label{fig:question2_2}
\end{figure}

\subsubsection{Section 3: Rank by comparison}
In section 3 of the survey, the users compared all 9 datasets and ranked them according to their photorealism(1 being the best rank).
Figures ~\ref{fig:question3} and ~\ref{fig:question3_2} show distribution and average ranking for each of the datasets.
The real dataset Pix3D got the highest number of votes for rank 1, while S2R:3D-FREE got least.
But if we have a threshold of 5, meaning frequency of votes being in top 5 ranks, then among the automated group, S2R:3D-FREE occurs most times followed by SceneNet, Openrooms and Blenderproc.
This is significant because out of 9 datasets, 4 are automated, and these four have the least average rankings.
But S2R:3D-FREE breaks the boundary to be in top 5 most times.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question3.pgf}}
    \caption{The figure represents distribution for Section 3 of survey. The participants were asked to rank the images based on photorealism(1 being the best) by comparing images from all 9 datasets.}
    \label{fig:question3}
\end{figure}

\begin{figure}
    \centering
    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question3_3.pgf}}
    \caption{The figure represents box plot for section 3. The Green horizontal line within the box indicates median, the blue dotted line indicates mean.}
    \label{fig:question3_2}
\end{figure}

\subsubsection{Summary for survey}
In the survey we observe that Pix3D was rightly chosen as photorealistic image as they are collected from real world data.
Though the proposed S2R:3DFREE dataset falls behind in comparison to manually designed images from Hyperism, InteriorNet and 3DFRONT,
it is comparable to automated datasets from Blenderproc, OpenRooms and SceneNet.
It also trumps over AI2THOR which is manually designed using Unity game engines.

\section{Domain gaps}\label{sec:domain-gaps}

In this section, we verify if the synthetic dataset;
S2R:3DFREE, has domain gap with the real dataset(Pix3D).
Along with new synthetic dataset, we will compare the datasets used for survey in ~\ref{sec:a-survey-on-photorealism}.
Qualitatively we will visualise dataset embeddings using T-SNE, and quantitatively we will compare the distributions of all the synthetic dataset and real dataset.

\subsection{Qualitative}\label{subsec:qualitative}
Visualize domain gap between S2R:3D-FREE and real image(pix3d)

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/out/images/tsne/photorealisit_dataset_2}
    \caption{T-SNE visualisation for images from various photo-realistic synthetic dataset. Pix3d and S2R3DFREE is highlighted with bolder colors.}
    \label{fig:photorealistic tsne}
\end{figure}

\subsection{Quantitative}\label{subsec:quantitative}
Maximum Mean Discrepancy
Kullback-Leibler divergence


\begin{center}
    \begin{tabular}{||c |c |c |c||}
        \hline
        Dataset & MSE & FID \\ [0.5ex]
        \hline\hline
        Openrooms & 7.63 & 189.43 \\
        \hline
         AI2THOR & 6.94 & 164.61 \\
        \hline
        BlenderProc & 5.53 & 173.37 \\
        \hline
        Hyperism & 7.12 & 186.57 \\
        \hline
        3DFRONT & 6.93 & 167.65 \\
        \hline
        InteriorNet & xx & xx \\
        \hline
        SceneNet & xx & xx \\
        \hline
        S2R:3DFREE & 1.86 & 178.83 \\[1ex]
        \hline
    \end{tabular}
\end{center}



\section{Datasets}\label{sec:datasets}
In this section, the datasets used for the following evaluations will be described.
The datasets are intended to have variations in domain randomisation to check its performance on 3D reconstruction task.

\subsection{Pix3D}
As mentioned in ~\ref{subsec:why-pix3d?}, we use a real dataset from ~\cite{pix3d} which is a collection of indoor scenes.
The 2 classes 'misc' and 'tools' are eliminated so that we focus only on furnitures.
The total images after the reduction is 9954 with 354 unique models.
The train and validation dataset is divided in the ratio of 70:30 giving us 6814 images from training and 3140 images for validation/test.
We do not have a test set only for this dataset since it is already limited and the validation set is used as test set while testing with synthetic data.
Samples are as shown in ~\ref{fig:samples for synthetic and real comparison}.

\subsection{S2R:3DFREE Version 1}
Version 1 of S2R:3DFREE was created by keeping the models in the center of a default 3D room.
The camera distance was randomised between 1 and 2.5 meters from the model.
The camera view points and textures were randomised.
A total of 70000 images were synthetically generated using the S2R:3DFREE 'Single Room pipeline' with 10000 images per category.
Samples are as shown in ~\ref{fig:samples for synthetic and real comparison}.

\subsection{S2R:3DFREE Version 2}
Version 1 of S2R:3DFREE was created by keeping the models in the center of a defualt 3D room.
The camera distance was randomised between 1 and 2.5 meters from the model.
The camera view points and textures were randomised.
A total of 21000 images were synthetically generated using the S2R:3DFREE 'Multi Object pipeline' with 3000 images per category.
Samples are as shown in ~\ref{fig:samples for synthetic and real comparison}.

\subsection{S2R:3DFREE Ablation}\label{subsec:s2r:3dfree-ablation}
To study the affects of parameters of domain randomisation, a study was conducted on chair dataset by omitting few factors one at a time.
This dataset were divided into 5 categories with different randomisation parameters.
The sample images with different randomization is as shown in figure ~\ref{fig:domain randomisation for ablation study}.

\subsubsection{2R:3DFREE\_Textureless}

    For textureless dataset, the chair models were kept at the center of an un-textured room with constant light source.
    A total of 10000 images were generated from different camera viewpoints.

\subsubsection{S2R:3DFREE\_Textureless\_Light}

    For this dataset, similar to the textureless dataset the chair models were kept at the center of an un-textured room, but with randomised light source.
    The light variation was implemented as in ~\ref{subsec:lightings-and-shadows}.
    Along with randomised light source, the camera view points were randomised with a distance in range of 0.75 to 1.5 meters from the model under observation.

\subsubsection{S2R:3DFREE\_Textured}

    As the name suggests, in this dataset both the model and the default single room were textured randomly for each snapshot as explained in ~\ref{subsec:randomised-texture}.
    10000 snapshot of chair models were taken using different camera viewpoints.

\subsubsection{S2R:3DFREE\_Textured\_Light}

    This is an extension of the above mention S2R:3DFREE\_Textured dataset, with addition of randomized light sources.
    The lights are randomized as implemented in ~\ref{subsec:lightings-and-shadows}.

\subsubsection{S2R:3DFREE\_Chair}

    S2R:3DFREE\_Chair dataset was created using 'Multi Object pipeline' with chair replacing a similar category from scene as implemented in ~\ref{subsec:replacing-target-objects}.
    Both the light and camera view points were randomised making sure that the model under observation is not completly occluded.


\begin{figure}
    \begin{tabular}{llll}
        Pix3D & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_3}\\

        S2R:3DFREE Version 1 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_3}\\

        S2R:3DFREE Version 2 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_3}\\

    \end{tabular}
    \caption{Samples of images from real and synthetic datasets.}
    \label{fig:samples for synthetic and real comparison}
\end{figure}


\begin{figure}
    \begin{tabular}{llll}
        Textureless & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_3}\\

        Textureless with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_3}\\

        Textured & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_1} &
        \includegraphics[width=.2\linewidthm]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_3}\\

        Textured with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_4}\\

        Multi-Object & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_3}\\

    \end{tabular}
    \caption{Samples of images used for ablation study on chairs with different parameters of domain randomisation.}
    \label{fig:domain randomisation for ablation study}
\end{figure}

Performance of model trained on synthetic dataset by testing model with real dataset(pix3d)
Qualitative: Voxel output comparisons
Quantitative: IOU, Dice

\section{Baseline}\label{sec:baseline}

As mentioned in ~\ref{subsec:pix2vox-and-pix2vox++}, pix2vox and pix2vox++ are the models which will act as the baselines for all the experiments.
For the dataset from ~\ref{sec:datasets}, Pix3D is the real dataset and will be acting as the base dataset.
The models are also compared with and without 2D augmentation.
The 2D augmentations include Random Flip, Random Crop, Color Jitter, RandomPermuteRGB.
A performance test with no augmentation will give us actually idea whether synthetic dataset actually enhances the performance of 3D reconstruction task.

Figure ~\ref{fig:baseline1}, represents performance of baseline models on different datasets.
It is seen that 2D augmentation improves IoU by 1.2\% for Pix3D on pix2vox++, and 1.15\% on pix2vox.
For synthetic dataset, the models are trained on 70\% of data and 30\% is used for validation.
For testing we use the same 30\% of real data from Pix3D dataset.
In case of synthetic dataset, 2D augmentation increased the IoU by 2.04\% and 4.77\%  for pix2vo++ and pix2vox respectively.
When it comes to the question of whether synthetic dataset gives equivalent performance as real dataset, we can clearly see a dip in the performnace,
proving that there is a domain gap between real and synthetic data.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline1.pgf}}
    \caption{Bar plot for the IoU for baseline models(Pix2Vox++ and Pix2Vox) trained on dataset mentioned in ~\ref{sec:datasets}. }
    \label{fig:baseline1}
\end{figure}

%\todo{replace value of s2r\_v1(currently values are from s2r\_v4)}

\section{Fine Tuning}\label{sec:fine-tuning}
Fine tuning or Transfer Learning is a common way of domain adaptation.
For this experiment, the model is first trained on synthetic dataset and then used as a pre-trained model to be fine tuned using real dataset.

In figure ~\ref{fig:finetuning1}, we have a comparison of IoU with pure real and pure synthetic dataset, followed by fine-tuning the models with real dataset.
The core comparison is between real data and fine-tuned model.
Models are pre-trained with 2 versions of S2R:3DFREE as mentioned in ~\ref{sec:datasets}.
It is noticed that for Version 1, there is an increment of 1.36\% and 1.55\% on pix2vox++ and pix2vox respectively.
For Version 2, an increment of 2.41\% on pix2vox++, but decrement of 1.01\% on pix2vox model.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/finetuning1.pgf}}
    \caption{Bar plot for the IoU for baseline models(Pix2Vox++ and Pix2Vox) trained on synthetic and fine-tuned with real dataset. }
    \label{fig:finetuning1}
\end{figure}

\section{Mixed Training}\label{sec:mixed-training}


\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed1.pgf}}
    \caption{Bar plot for the IoU for baselines trained on different ratios of synthetic and real dataset}
    \label{fig:mixed1}
\end{figure}

\section{Ablation study on chairs}\label{sec:ablation-study-on-chairs}
In this section, we conduct ablation study oon chairs by changing domain randomisation property.
The dataset used for the study is explained in subsection ~\ref{subsec:s2r:3dfree-ablation}.
The samples for dataset used is as shown in ~\ref{fig:domain randomisation for ablation study}.
For comparison with real dataset, we extract only the chair models from Pix3D and compare the results of with and without 2D augmentation with synthetic dataset.

In figure ~\ref{fig:ablation1}, we see that textureless chair dataset out performs rest of the dataset.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation1.pgf}}
    \caption{Bar plot for the IoU for baseline trained on chair dataset with different parameters and tested on real dataset.}
    \label{fig:ablation1}
\end{figure}
\todo{make sure bar values are not overlapping}



\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation2.pgf}}
    \caption{Bar plot for the IoU for baseline trained by mixing chair dataset from real and synthetic dataset with ratio of 50\%}
    \label{fig:ablation2}
\end{figure}

\subsection{Domain randomisation on chair dataset}
Check if randomising textures of chair and background helps improve performance.  Example: Create dataset with constant room texture vs randomising texture,
Check if lighting helps (indoor, outdoor). Example: Create a dataset with constant lighting(only outdoor lighting) vs indoor lighting.
Iou per category: as in https://arxiv.org/pdf/1905.03678.pdf

\subsection{Domain randomisation with Mixed training}


