\chapter{\iftoggle{german}{Evaluierung}{Experiments and Evaluation}}\label{ch:evaluation}

%\todo{
%    \begin{enumerate}
%        \item baseline comparison, pix3d pix2vox, pix2vox++ classwise)
%        \item baseline different version of dataset with different threshold
%        \item ablation values per model
%        \item !!!!!finetuning with different datasize (pending)
%        \item !!!!!mixed training with different real datasize(pending)
%        \item training graphs
%    \end{enumerate}
%}

This chapter conducts experiments that will help us find solutions for the research questions discussed in \autoref{sec:goal}.
\autoref{sec:a-survey-on-photorealism} will contain the survey results conducted to check the photorealism of the proposed synthetic dataset.
This section will also compare the ratings given to other proclaimed photorealistic datasets and check whether the \gls{free} dataset compares to those datasets.
We will further evaluate the datasets using a \gls{tsne} as a qualitative measure and \gls{fid} as a quantitative measure which will indicate the domain gaps concerning the real dataset.

\autoref{sec:datasets}, describes different datasets specifically generated to evaluate our baseline models and check randomization parameters.
\autoref{sec:baseline} evaluates baseline models with real and synthetic versions of datasets.
\autoref{sec:fine-tuning} further evaluates models pre-trained on synthetic datasets by fine-tuning them using a real dataset.
\autoref{sec:mixed-training} deals with mixed training and its impact on performance with different mixing ratios of the real and synthetic datasets per mini-batch.
\autoref{sec:ablation-study-on-chairs}, will evaluate models on chair datasets with different randomization parameters and mixed training for these individual datasets.

\section{A survey on photorealism}\label{sec:a-survey-on-photorealism}

The participants received minimalistic information about the intention behind the survey.
The goal of the survey was to analyze if humans have the same perception of photographic and computer-generated images.
The survey was open to everyone with no time limit.
A total of 72 participants responded to the survey.
The survey was created using Google Forms, and the link was distributed.
The participants either used a mobile phone or a desktop to respond to the survey.
The survey constituted a total of 9 datasets.
\gls{front}~\cite{Fu20203DFRONT3F}, Hypersim~\cite{Roberts2020HypersimAP}, InteriorNet~\cite{InteriorNet18}, SceneNet~\cite{McCormac:etal:ICCV2017}, BlenderProc~\cite{denninger2019blenderproc},
\gls{ai2thor}~\cite{kolve2019ai2thor}, Openroom~\cite{li2021openrooms}, Pix3D~\cite{pix3d} and proposed \gls{free} dataset.
Only Pix3D was a real dataset, while all others are synthetic datasets proclaimed to be photorealistic.

The survey was composed of 3 sections.
\begin{enumerate}
    \item Section 1: Decide if the image is real or not real.

    In this section, there were 27 images, three each from the datasets as mentioned earlier.
    Each image had only two options to select: "Real" or "Not real"
    This approach eliminated any ambiguous perception towards the images.

    \item Section 2: Rate the image on a scale of 1 to 10 in terms of realism (1 -> least real, 10 -> most real).

    In this section, the participant used a Likert scale to rate the images based on photorealism.
    Similar to section 1, there were 27 questions of three images per dataset.

    \item Section 3: Rank the images from 1 to 9 (1 -> Most real, 9 -> Least real).

    In this section, the participant had only three questions, with each question having an image from each of the datasets arranged in a 3\x3 grid format.
    The users ranked them in the increasing order of the photorealism.
\end{enumerate}

\subsection{Survey results}\label{subsec:survey-results}
In this segment, we discuss the results of the survey collected from participants.

\subsubsection{Section 1: Real or Not}
In section 1, the participants had only two options to select from independently.
\autoref{fig:question1} shows that the real dataset Pix3D~\cite{pix3d} was rightly recognized as real.
77\% of the real images that belonged to Pix3D were perceived to be real.
This shows that the participants were not convinced even with the real images, as 23\% were still recognized as not real.
Hyeperism~\cite{Roberts2020HypersimAP} got the best results of 59.7\% identified as real among the synthetic datasets,.
\gls{ai2thor} had the least amount of images recognized as real with just 5\% positive responses.
The proposed \gls{free} dataset had 8\% of images identified as real.
These two datasets were built in Unity, which shows that images generated using the automated Unity framework needs some improvement.
Suppose we have a threshold of 50\%, we see that the datasets for which the images selected are Not real below the threshold value belongs to datasets that are automated and not created by professionals manually.
As mentioned in \autoref{subsec:indoor-synthetic-datasets}, Openrooms, SceneNet, and Blenderproc are datasets obtained from automation.
We consider the \gls{free} dataset to be automated and belong to this category.
Openrooms have the most confidence votes among the automated images, with 37\% recognized as real images.
Even though \gls{free} was recognized as real the least number of times among the automated tools, it had better percentage than AI2THOR
which has a Unity-based framework to generate images and was manually configured by professionals by taking in reference of real-world images.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question1.pgf}}
    \caption{The figure represents distribution for Section 1 of survey. The participants were asked to distinguish if the image was 'Real' or 'Nor Real'.
    The automated dataset is highlighted with bolder color. The dotted line is 50\% threshold. All the automated dataset have less than threshold votes for 'Real'.}
    \label{fig:question1}
\end{figure}

\subsubsection{Section 2: Likert Scale}
In section 2, the participants could select ratings from 1 to 10 (1 being the least photorealistic).
The distribution of values for each dataset can be seen in \autoref{fig:question2}, and the average ratings are as seen in \autoref{fig:question2_2}.
If we consider the scale of 1, which is the least rating given to the image, \gls{ai2thor} has the most votes.
Suppose we have a cut-off at scale of 2 and 3; Openrooms and \gls{free} are the least photorealistic, respectively.
Interestingly Interiornet has the least number of scale 1 instead of Pix3D, which is the real dataset.
However, Pix3D has the highest number of the perfect score(10) among all the datasets.
Coming to the averages, we again see the datasets created from the automated pipeline (Blenderproc, SceneNet, Openrooms, \gls{free}), have the least average ratings,
while the manually created datasets(Hyperism, \gls{front}, InteriorNet) have higher average ratings.
Pix3D has highest of the average ratings, closely followed by InteriorNet.
Even though \gls{free} has the least average, it is still comparable to other automated pipelines as highlighted in \autoref{fig:question2_2} and even the Unity-based \gls{ai2thor} dataset.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question2.pgf}}
    \caption{The figure represents distribution for Section 2 of survey. The participants were asked to rate the image based on photorealism(1 being the least photorealistic).
    \gls{free} has maximum number of least rating(1), but it is comparable to other automated datasets as highlighted. Pix3D has maximum number of highest rating(10).}
    \label{fig:question2}
\end{figure}

\begin{figure}
    \centering
    \resizebox{0.75\textwidth}{10cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question2_2.pgf}}
    \caption{The figure represents average rating given by the participant to each of the datasets in section 2 of the survey.
    The automated datasets are highlighted, all of them have lower average. \gls{ai2thor}(Unity based and manually designed) is also among the lower averages.}
    \label{fig:question2_2}
\end{figure}

\subsubsection{Section 3: Rank by comparison}
In section 3 of the survey, the users compared all nine datasets and ranked them according to their photorealism(1 being the best rank).
\autoref{fig:question3} and \autoref{fig:question3_2} show distribution and average ranking for each of the datasets.
The real dataset Pix3D got the highest number of votes for rank 1, while \gls{free} got the least.
However, if we have a threshold of 5, meaning frequency of votes being in the top 5 ranks, then among the automated group, \gls{free} occurs most times, followed by SceneNet, Openrooms, and Blenderproc.
This investigation is significant because out of the nine datasets, four are automated, and these four have the least average rankings.
However, \gls{free} breaks the boundary to be in the top 5 most times.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question3.pgf}}
    \caption{The figure represents distribution for Section 3 of survey. The participants were asked to rank the images based on photorealism(1 being the best) by comparing images from all 9 datasets.
    The automated datasets are highlighted. \gls{free} dataset appears in top 5 maximum number of times, seen to the left of the dotted line.}
    \label{fig:question3}
\end{figure}

\begin{figure}
    \centering
    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question3_3.pgf}}
    \caption{The figure represents box plot for section 3. The orange horizontal line within the box indicates median, the blue dotted line indicates mean.
    The bolder boxes represents automated dataset, while lighter boxes represent manually created dataset. \Gls{free} has highest mean among the automated datasets.}
    \label{fig:question3_2}
\end{figure}

\subsubsection{Summary for survey}
In the survey, we observe Pix3D was rightly chosen as a photorealistic image because it is a real-world image collection.
Though the proposed \gls{free} dataset falls behind in comparison to manually designed images from Hyperism, InteriorNet, and \gls{front},
it is comparable to automated datasets from Blenderproc, OpenRooms and SceneNet.
It also trumps over AI2THOR, which is manually designed using Unity game engines.

\section{Domain gaps}\label{sec:domain-gaps}

This section verifies if the synthetic dataset; \gls{free} has a domain gap with the real dataset(Pix3D).
Along with the new synthetic dataset, we will compare the datasets used for the survey in \autoref{sec:a-survey-on-photorealism}.
Qualitatively we will visualize dataset embeddings using \gls{tsne} in \autoref{subsec:qualitative}; we will compare the distributions of all the synthetic datasets and the real dataset as in \autoref{subsec:quantitative}.

\subsection{Qualitative}\label{subsec:qualitative}

For qualitative assessment of the domains for each dataset, we utilize \gls{tsne} visualizations of embedding space from \gls{vgg}~\cite{simonyan2015deep}.
We consider a \gls{vgg}16 model pre-trained on ImageNet~\cite{Deng2009ImageNetAL} and use it as an encoder to embed the image space of all the images from each dataset.
This latent space embedding is then converted to 2-Dimensional representation using \gls{tsne} visualization.
A model trained on ImageNet can be used for encoding the images since it contains all the furniture categories present in Pix3D\@.
Hence the images will be respectfully embedded and mapped to 2D space.
As indicated in \autoref{subsec:visualizing-with-tsne}, the distances of the clusters, here dataset, is not represented by the \gls{tsne} visualization.
However, the overlap can be considered as an inference of occupying the same latent space.

\begin{figure}[ht]
    \centering
    \resizebox{0.9\textwidth}{8.5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/photorealisitic_dataset_2.pgf}}
    \caption{\gls{tsne} visualization for images from various photo-realistic synthetic dataset. Pix3D and \gls{free} are highlighted with bolder colors.
    Both these datasets have wide spread in the embedding space.}
    \label{fig:photorealistic tsne}
\end{figure}

\begin{figure}[!ht]
    \centering
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/openrooms_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/scenenet_Pix3d.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/blenderproc_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/ai2thor_Pix3d.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/3dfront_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/hypersim_Pix3d.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/interiornet_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/s2r-3dfree_Pix3d.pgf}}\\
    \caption{\gls{tsne} visualization for images from individual photo-realistic synthetic dataset compared with Pix3D latent space.
        (Left to right, top to bottom) Openrooms, SceneNet, Blenderproc, \gls{ai2thor}, \gls{front}, Hyperism, InteriorNet, \gls{free} in blue;
        compared with Pix3D in orange.}
    \label{fig:tsne per dataset}
\end{figure}

For each dataset, 30 randomly chosen images were passed through the encoder at the same time.
The images are less in quantity because not all datasets provide images directly.
Some datasets like Hyperism, SceneNet, Openrooms are built for training \gls{slam} models, and thus not all frames contain furniture.
We had to filter the images containing furniture to appear to be in the same embedding space.

In \autoref{fig:photorealistic tsne}, we see the embedding for each dataset used for the survey discussed in \autoref{sec:a-survey-on-photorealism}.
For better visualization, the points are connected to the centroid of the dataset;
if not, we found it challenging to comprehend the scatterplot of all datasets in a single graph.
The proposed \gls{free} and the real dataset, Pix3D, are highlighted for better focus.

Each of the dataset have considerable overlap of latent space as seen in \autoref{fig:tsne per dataset}, which indicate that each of the dataset is closer to the photorealism of a real-world dataset.
From the \gls{tsne} visualization in \autoref{fig:photorealistic tsne}, we see that the real dataset(Pix3D) is spread across the space and lies at the center of the plot.
Interestingly, all the automated datasets(Openrooms, SceneNet, Blenderproc, \gls{free}) form cluster close to each other, while the non-automated datasets(Hyperism, InteriorNet, \gls{front}, \gls{ai2thor}) form cluster on the other side.
\gls{ai2thor}, openrooms, Hyperism, and \gls{front} have their embedding mapped in the outer region.
Openrooms seems to have least overlap with Pix3D which could indicate a gap in the two domains.
\gls{free} has a widespread and occupies much of the space occupied by Pix3D.
This can also be seen in \autoref{fig:pix3d_s2r3dfree}.
For this visualization, 280  images were randomly selected from each dataset, with 40 images belonging to each category.
However, the \gls{free} needs more randomization to encapsulate the latent space of Pix3D for better performance.

\begin{figure}[ht]
    \centering
    \resizebox{0.75\textwidth}{8.5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/s2r3dfree_Pix3d280.pgf}}
    \caption{\gls{tsne} visualization for images from Pix3D and \gls{free} dataset.We observe that \gls{free} still doesnt encapsulate the embedding space like Pix3D.}
    \label{fig:pix3d_s2r3dfree}
\end{figure}


We plot the visualization for each dataset individually with the real dataset in \autoref{fig:tsne per dataset}.
From this figure, we can observe that the latent space of the real dataset and \gls{free} are overlapping and not clustered separately.
We can also see that \gls{ai2thor} has the least common latent space, while Blenderproc has the maximum.
Openrooms and SceneNet also have latent space significantly apart from the real dataset, while Hyperism and \gls{front} have a widespread space that intersects the real data.

To summarize, all the proclaimed photorealistic datasets used in \autoref{sec:a-survey-on-photorealism}, have atleast some latent space common with the real dataset.
Hence, we could see a discrepancy in the user survey wherein the participants had a confused perspective of photorealism.

\subsection{Quantitative}\label{subsec:quantitative}

%We visualized the embedding space using \gls{tsne} in above \autoref{subsec:qualitative}.
%All the datasets seem to have a very close relationship with the real dataset, as we see at least some points being in the intersection with the latent space of the real dataset.
%This section compares the datasets with quantitative assessment using \gls{mse} and \gls{fid}.
%As seen in \autoref{tab:quantitative-dataset-comparison}, BlenderProc has the least MSE of 5.53 to Pix3D, while Openrooms have the highest MES of 7.63.
%\gls{free} has a MSE of 7.075, which is below Openrooms and Hyperism.
%Interestingly, \gls{ai2thor} seems to perform better in quantitative assessment of photorealism,
%where both MSE and \gls{fid} are considerably lesser than other datasets,
%establishing that \gls{ai2thor} is closer to the real dataset than what was visualized in \autoref{fig:photorealistic tsne} and \autoref{fig:tsne per dataset}.
%\gls{free} has an \gls{fid} of 178.83, which is lesser than Openrooms, Hyperism, and SceneNet.
%
%\begin{table}[ht]
%    \centering
%    \begin{tabular}{|c |c |c |c|}
%        \hline
%        Dataset & \gls{mse} & \gls{fid} \\ [0.5ex]
%        \hline\hline
%        Openrooms & 7.63 & 189.43 \\
%        \hline
%        \gls{ai2thor} & 6.94 & 164.61 \\
%        \hline
%        BlenderProc & 5.53 & 173.37 \\
%        \hline
%        Hyperism & 7.12 & 186.57 \\
%        \hline
%        \gls{front} & 6.93 & 167.65 \\
%        \hline
%        InteriorNet & 6.5828 & 160.87 \\
%        \hline
%        SceneNet & 6.7553 & 185.49 \\
%        \hline
%        \gls{free} & 7.0750 & 178.83 \\[1ex]
%        \hline
%    \end{tabular}
%    \caption{Table represents quantitative measure to compare synthetic dataset distribution with the real dataset(Pix3D)}
%    \label{tab:quantitative-dataset-comparison}
%\end{table}

We visualized the embedding space using \gls{tsne} in above \autoref{subsec:qualitative}.
All the datasets seem to have a very close relationship with the real dataset, as we see at least some points being in the intersection with the latent space of the real dataset.
This section compares the datasets with quantitative assessment using \gls{fid}.
\gls{fid} is calculated as explained in \autoref{subsec:fr'echet-inception-distance)}.
For any two distributions or datasets to be similar we expect the \gls{fid} to be diminutive.

As seen in \autoref{tab:quantitative-dataset-comparison}, InteriorNet has the least \gls{fid} of 160.87 to Pix3D, while Openrooms have the highest \gls{fid} of 189.43.
Interestingly, \gls{ai2thor} seems to perform better in quantitative assessment of photorealism,
where \gls{fid} are considerably lesser than other datasets,
establishing that \gls{ai2thor} is closer to the real dataset, contrary to what the survey results in \autoref{subsec:survey-results} concluded.
\gls{free} has an \gls{fid} of 178.83, which is lesser than Openrooms, Hyperism, and SceneNet.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c |c |}
        \hline
        Dataset & \gls{fid} \\ [0.5ex]
        \hline\hline
        Openrooms & 189.43 \\
        \hline
        \gls{ai2thor} & 164.61 \\
        \hline
        BlenderProc  & 173.37 \\
        \hline
        Hyperism  & 186.57 \\
        \hline
        \gls{front}  & 167.65 \\
        \hline
        InteriorNet  & 160.87 \\
        \hline
        SceneNet & 185.49 \\
        \hline
        \gls{free} & 178.83 \\[1ex]
        \hline
    \end{tabular}
    \caption{Table represents quantitative - \gls{fid} measure to compare synthetic dataset distribution with the real dataset(Pix3D)}
    \label{tab:quantitative-dataset-comparison}
\end{table}

\section{Datasets}\label{sec:datasets}
In this section, we describe the datasets used for the following evaluations.
Datasets intend to have variations in domain randomization to check their performance on the 3D reconstruction tasks.

\subsection{Pix3D}\label{subsec:pix3d}
As mentioned in \autoref{subsec:why-pix3d?}, we use a real dataset from~\cite{pix3d}, a collection of indoor scenes.
The two classes ’misc’ and ’tools’ are eliminated to focus only on furniture.
The total images after the reduction are 9954 with 354 unique models.
The dataset is divided into 70:30, giving us 6814 images from training and 3140 images for validation/test.
We do not have a test set only for this dataset since it is already limited, and the validation set is used as a test set while testing with synthetic data.
Samples are as shown in \autoref{fig:samples for synthetic and real comparison}.

\subsection{\gls{s2rv1}}\label{subsec:gls{free}-version-1}
Version 1 of \gls{free} was created by keeping the models in the center of a default 3D room.
The camera distance was randomized between 1 and 2.5 meters from the model.
The camera viewpoints and textures were randomized.
A total of 70000 images were synthetically generated using the \gls{free} 'Single Room pipeline' with 10000 images per category.
Samples are as shown in \autoref{fig:samples for synthetic and real comparison}.

\subsection{\gls{s2rv2}}\label{subsec:gls{free}-version-2}
Version 1 of \gls{free} was created by keeping the models in the center of a defualt 3D room.
The distance of the camera was randomly chosen between 1 and 2.5 meters from the model.
The camera view points and textures were randomized.
A total of 21000 images were synthetically generated using the \gls{free} 'Multi-Object pipeline' with 3000 images per category.
Samples are as shown in \autoref{fig:samples for synthetic and real comparison}.

\begin{figure}[ht]
    \centering
    \begin{tabular}{llll}
        Pix3D & \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_1} &
        \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_2} &
        \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_3}\\

        \gls{s2rv1} & \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_1} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_2} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_3}\\

        \gls{s2rv2} & \includegraphics[width=.19\textwidth, height =.2\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_1} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_2} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_3}\\

    \end{tabular}
    \caption{Samples of images from real and synthetic datasets.}
    \label{fig:samples for synthetic and real comparison}
\end{figure}

\subsection{\gls{free} Ablation}\label{subsec:s2r:3dfree-ablation}
Pix3D is composed of 3839 chairs which is the maximum among all categories.
To study the effects of domain randomization, we create multiple chair datasets by omitting randomizing factors one at a time and study the model behavior.
We divide the datasets into five categories with different randomization parameters.
The sample images with different randomization are as shown in \autoref{fig:domain_randomisation_for_ablation_study}.

\autoref{fig:tsne per chair dataset} represents latent space for each variation of domain randomization for chairs compared to chairs from the real dataset(Pix3D).
Two hundred images were randomly sampled from each dataset and embedded using pre-trained \gls{vgg} as in \autoref{subsec:qualitative}.
The last \gls{tsne} representation in \autoref{fig:tsne per chair dataset} shows a combined latent space of all the randomized datasets for chairs.
We see that combined space is close to the real dataset space and spreads across the latent space, while individual parameters have limited spread.

In \autoref{tab:quantitative-dataset-comparison-chair-dataset}, we see the corresponding quantitative measure with \gls{fid} for 100 randomly chosen images from each dataset compared to Pix3D chair dataset.
We see that the textureless dataset has an \gls{fid} of 125.91.
When light is added to this dataset, the \gls{fid} increases to 136.78.
Similarly, for textured dataset \gls{fid} is 146.48 and with light added it increases to 155.76.
We establish that light plays a major role for the datasets to be similar, but since it is showing a negative impact, we need a better control over its parameters.
Multi-Object dataset has the best similarity with least \gls{fid} of 123.45, which reflects in its performance in \autoref{sec:ablation-study-on-chairs}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c |c |}
        \hline
        Dataset & \gls{fid} \\ [0.5ex]
        \hline\hline
        Textureless & 125.91 \\
        \hline
        Textureless with light & 136.78 \\
        \hline
        Textured  & 146.48 \\
        \hline
        Textured with light  & 155.76 \\
        \hline
        Multi-Object & 123.45 \\[1ex]
        \hline
    \end{tabular}
    \caption{Table represents quantitative - \gls{fid} measure to compare chair synthetic dataset distribution with the real dataset(Pix3D)}
    \label{tab:quantitative-dataset-comparison-chair-dataset}
\end{table}

\begin{figure}[!ht]
    \centering
    \resizebox{0.49\linewidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_textureless.pgf}}
    \resizebox{0.49\linewidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_textureless_light.pgf}}\\
    \resizebox{0.49\linewidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_textured.pgf}}
    \resizebox{0.49\linewidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_textured_light.pgf}}\\
    \resizebox{0.49\linewidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_multi-object.pgf}}
    \resizebox{0.49\linewidth}{6cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_chair_DR.pgf}}\\
    \caption{\gls{tsne} visualization for images from individual synthetic chair dataset with different domain randomization parameter compared with Pix3D chair latent space.
        (Left to right, top to bottom) Textureless, Textureless with light, Textured, Textured with light, Multi-Object and Combined}
    \label{fig:tsne per chair dataset}
\end{figure}


\begin{figure}[!ht]
    \begin{tabular}{llll}
        Textureless & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_3}\\

        Textureless with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_3}\\

        Textured & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_3}\\

        Textured with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_4}\\

        Multi-Object & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_3}\\

    \end{tabular}
    \caption{Samples of images used for ablation study on chairs with different parameters of domain randomization.}
    \label{fig:domain_randomisation_for_ablation_study}
\end{figure}

%\begin{figure}
%    \centering
%    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_chair_DR.pgf}}
%    \caption{A combined \gls{tsne} visualization for chair images from Pix3d and \gls{free} dataset with different parameters for randomization.}
%    \label{fig:pix3dchair_s2r3dfreechair}
%\end{figure}

\subsubsection{2R:3DFREE\_Textureless}

    The chair models were kept at the center of an un-textured room with a constant light source for the textureless dataset.
    A total of 10000 images were generated from different camera viewpoints.

\subsubsection{\gls{free}\_Textureless\_Light}

    Like the textureless dataset, the chair models were kept at the center of an un-textured room for this dataset.
    However, the light source was randomized.
    The light variation was implemented as in \autoref{subsec:lightings-and-shadows}.
    Along with randomized light sources, the camera viewpoints were randomized with a distance in the range of 0.75 to 1.5 meters from the model under observation.

\subsubsection{\gls{free}\_Textured}

    As the name suggests, both the model and the single default room were textured randomly for each snapshot, as explained in \autoref{subsec:randomised-texture}.
    Ten thousand snapshots of chair models were taken using different camera viewpoints.

\subsubsection{\gls{free}\_Textured\_Light}

    This dataset is an extension of the above mention \gls{free}\_Textured dataset, with randomized light sources.
    The lights are randomized as implemented in \autoref{subsec:lightings-and-shadows}.

\subsubsection{\gls{free}\_Mutli-Object}

    \gls{free}\_Mutli-Object dataset was created using 'Multi-Object pipeline' with chair replacing a similar category from the scene as implemented in \autoref{subsec:replacing-target-objects}.
    Both the light and camera viewpoints were randomized, ensuring that the model under observation is not completely occluded.

\subsubsection{\gls{free}\_Combined}
    \gls{free}\_Combined dataset is a combination of all the five types of datasets mentioned above.
    Adding 10,000 images from each dataset, we get a total of 50,000 chair images in this bundle.
    This dataset is an aggregate of all types of domain randomization.

\section{Baseline}\label{sec:baseline}

As mentioned in \autoref{subsec:pix2vox-and-pix2vox++}, pix2vox and pix2vox++ are the models which will act as the baselines for all the experiments.
For the dataset from \autoref{sec:datasets}, Pix3D is the real dataset and will be acting as the base dataset.
The models are also compared with and without 2D augmentation.
The 2D augmentations include Random Flip, Random Crop, Color Jitter, RandomPermuteRGB\@.
A performance test with no augmentation will explain the importance of 2D augmentation in the performance of the 3D reconstruction tasks.
\autoref{fig:baseline1} represents the performance of baseline models on different datasets, both with and without data augmentation.
For the validation step, we save two checkpoints for the best epoch.
One is validated with the real dataset, and the other is validated with the respective synthetic dataset.

The models are trained on 70\% of the data for the synthetic dataset, and 30\% is used for validation.
For testing, we use the same 30\% of real data from the Pix3D dataset.
For the real dataset, the models are trained on 70\% of data and validated/tested on the same 30\% used in testing models trained on synthetic data.

\begin{figure}[ht]
    \centering
    \resizebox{0.49\linewidth}{0.5\linewidth}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline_barplot1.pgf}}
    \resizebox{0.49\linewidth}{0.5\linewidth}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline_barplot2.pgf}}
    \caption{Bar plot for the \gls{iou}  for \textbf{baselines} trained on real and synthetic datasets, with and without 2D augmentation.
        (left)The checkpoint was saved using real dataset for validation and test, (right) the checkpoint was saved using corresponding synthetic data for validation step and tested with real data.
        In both the cases we see that ~\gls{free} does not perform adequately on its own. \gls{s2rv2} contributes better than \gls{s2rv1}.}
    \label{fig:baseline1}
\end{figure}

\autoref{fig:baseline1} is a comparison of models trained with 2D augmentation, synthetic dataset, and real dataset.
For models validated with the real dataset, it is seen that 2D augmentation improves \gls{iou}  by 8.06\% for Pix3D on pix2vox++ and 1.22\% on pix2vox.
In the case of the synthetic dataset, 2D augmentation increases the \gls{iou}  by 1.42\% and 6.66\% for pix2vox++ and pix2vox, respectively.
When it comes to whether a synthetic dataset gives an equivalent performance as a real dataset, we can see a dip in the performance, demonstrating that there is a domain gap between real and synthetic data.
Out of the two synthetic datasets, \gls{s2rv2} gives better results than \gls{s2rv1} when tested with real data.
We hypothesize that since Pix3D has multi-objects in the scenes, same as \gls{s2rv2}, it performs better than single object images from \gls{s2rv1}.
A similar observation is seen when a checkpoint is saved with the synthetic dataset itself being the validation set as in \autoref{fig:baseline1}(right).


\begin{figure}[!ht]
    \begin{tabular}{llll}
        Pix3D images & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/table2}\\

        Ground Truth & \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/bed1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/sofa1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/table2_original}\\

        Output1 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2vpp_bed1_output} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2vpp_sofa1_output} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2vpp_table2}\\

        Output2 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2v_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2v_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2v_table2}\\

        Output3 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/s2rv3_p2vpp_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/s2rv3_p2vpp_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/s2rv3_p2vpp_table2}\\

        Output4 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/s2rv3_p2v_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/s2rv3_p2v_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/s2rv3_p2v_table2}\\

    \end{tabular}
    \caption{3D reconstruction outputs for models trained on \textbf{only real dataset}, and \textbf{only synthetic dataset}. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D(real dataset).
    Output3-4: Pix2Vox++ and Pix2Vox trained with only \gls{s2rv2} synthetic dataset. This corresponds to the bad \gls{iou} when trained on only synthetic dataset.}
    \label{fig:baseline_images1}
\end{figure}

In \autoref{fig:baseline_images1}, we see the 3D reconstruction output for models trained on only real and only synthetic datasets.
The outputs were collected for images from the real dataset with the threshold, which gave the best \gls{iou}.
The output of models trained on only synthetic holds to the \gls{iou} values seen in \autoref{fig:baseline1}.
The reconstructions show that the model has a domain gap, and hence the output does not match the ground truth.

%\begin{figure}
%    \centering
%    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline_linegraph1.pgf}}
%    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline_linegraph2.pgf}}
%    \caption{Line plot for the \gls{iou}  for baselines trained on real and synthetic datasets, with and without 2D augmentation.
%        (Left)The checkpoint was saved using real dataset for validation and test, (right) the checkpoint was saved using corresponding synthetic data for validation step and tested with real data.
%        In both the cases we see that ~\gls{free} does not perform adequately on its own. \gls{s2rv2} contributes better than \gls{s2rv1}.}
%    \label{fig:baseline1}
%\end{figure}


\section{Fine Tuning}\label{sec:fine-tuning}
Fine-tuning or Transfer Learning is a common way of domain adaptation.
For this experiment, the model is first trained on a synthetic dataset and then used as a pre-trained model to be fine-tuned using a real dataset.

In \autoref{fig:finetuning1}, compares \gls{iou} of a pure real and a pure synthetic dataset, followed by fine-tuning the models with the real dataset.
The core comparison is between real data and fine-tuned model.
Models are pre-trained with two versions of \gls{free}(\gls{s2rv1} and \gls{s2rv2}) as mentioned in \autoref{sec:datasets}.
It is noticed that for \gls{s2rv1}, there is a decrement of 3.04\% and 1.25\% on pix2vox++ and pix2vox, respectively.
For \gls{s2rv2}, a decrement of 3.54\% on pix2vox++, but an increment of 0.8\% on the pix2vox model.

\begin{figure}[ht]
    \centering
    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/finetuning_barplot1.pgf}}
    \caption{Bar plot for the \gls{iou} for baseline models(Pix2Vox++ and Pix2Vox) trained on synthetic and \textbf{fine-tuned} with real dataset.
    We see that even after fine-tuning both the models do not perform as good as models trained on only real dataset.}
    \label{fig:finetuning1}
\end{figure}


\begin{figure}[!ht]
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/finetune_parallel_pix2voxpp.pgf}}
    \caption{Parallel coordinate plot for the \gls{iou} for baseline \texbf{pix2vox++} trained on (\gls{s2rv1}, \gls{s2rv2}) and \textbf{fine-tuned} with pix3d.
    The categories are listed along with the number of images.
    The performance of \texbf{pix2vox++} mixed with both the synthetic dataset is less than model trained on only pix3d, for majority of the categories.}
    \label{fig:finetuning2}
\end{figure}

\begin{figure}[!ht]c
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/finetune_parallel_pix2vox.pgf}}
    \caption{Parallel coordinate plot for the \gls{iou} for baseline \texbf{pix2vox} trained on (\gls{s2rv1}, \gls{s2rv2}) and \texbf{fine-tuned} with pix3d.
    The categories are listed along with the number of images.
    The performance of \textbf{pix2vox} mixed with both the synthetic dataset is less than model trained on only pix3d, for majority the categories.}
    \label{fig:finetuning3}
\end{figure}

In \autoref{fig:finetuning2} and \autoref{fig:finetuning3}, we observe the \gls{iou} for each category, for pix2vox++ and pix2vox, respectively.
We can see that model trained on only Pix3D has more \gls{iou} for most of the categories on average.
For pix2vox, \gls{s2rv2} has a slight gain over Pix3D in some categories.
Overall, fine-tuning is not helping in increasing the performance of the baseline models.


\begin{figure}[!ht]
    \begin{tabular}{llll}
        Pix3D images & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/table2}\\

        Ground Truth & \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/bed1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/sofa1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/table2_original}\\

        Output1 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2vpp_bed1_output} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2vpp_sofa1_output} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2vpp_table2}\\

        Output2 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2v_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2v_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/pix3d_p2v_table2}\\

        Output3 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/finetuning/s2rv3_p2vpp_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/finetuning/s2rv3_p2vpp_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/finetuning/s2rv3_p2vpp_table2}\\

        Output4 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/finetuning/s2rv3_p2v_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/finetuning/s2rv3_p2v_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/finetuning/s2rv3_p2v_table2}\\

    \end{tabular}
    \caption{3D reconstruction outputs for models trained on real dataset and synthetic datasets with \textbf{fine-tuning}. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D(real dataset).
    Output3-4: Pix2Vox++ and Pix2Vox pre-trained with only \gls{s2rv2} synthetic dataset and then fine-tuned with Pix3D. The reconstruction is better than models trained on only synthetic dataset.}
    \label{fig:finetuning_images1}
\end{figure}

In \autoref{fig:finetuning_images1}, we see the 3D reconstruction output for models trained on synthetic and fine-tuned with real.
The outputs were collected for images from the real dataset with the threshold, which gave the best \gls{iou}.
The output of models improved over models trained on only synthetic dataset as in \autoref{fig:baseline_images1}.
When compared to models trained on only real dataset as in \autoref{fig:finetuning_images1}, the output is noisier with less detailed output.


%\begin{figure}
%    \centering
%    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/finetuning_linegraph1.pgf}}
%    \caption{Line plot for the \gls{iou} for baseline models(Pix2Vox++ and Pix2Vox) trained on synthetic and fine-tuned with real dataset.
%    We see that even after fine-tuning both the models do not perform as good as models trained on only real dataset.}
%    \label{fig:finetuning1}
%\end{figure}


\section{Mixed Training}\label{sec:mixed-training}
For mixed training, we mix synthetic and real datasets with a fixed ratio in each mini-batch.
The ratios used were \emph{0.15, 0.25, 0.5, 0.75 and 0.9}.
The higher the ratio, the closer the mixed dataset becomes a real dataset.
Both the real and synthetic datasets were drawn according to these ratios for each mini-batch.
The synthetic dataset is much more abundant than the real, so the real dataset will be oversampled to achieve the mentioned ratios.

In \autoref{fig:mixed1}, models trained with each of the ratios have better performance than the baseline of 0.3443 for pix2vox++,
except for a ratio of 15\%, which in some cases has a slightly lower \gls{iou}  value.
The most significant difference is seen at 50\% using the \gls{s2rv2} dataset with an increase of 3.04\%.
We saw an increase in \gls{iou} for pix2vox rather than pix2vox++.
An increase of 2.62\% is noted for the 25\% mix with \gls{s2rv2} on pix2vox.

\begin{figure}[ht]
    \centering
    \resizebox{0.49\linewidth}{0.45\linewidth}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_barplot1.pgf}}
    \resizebox{0.49\linewidth}{0.45\linewidth}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_barplot2.pgf}}
    \caption{Bar plot for the \gls{iou} for baselines trained on different ratios of synthetic and real dataset per mini-batch.(left)\textbf{Mixed training on Pix2Vox++}, (right)\textbf{Mixed training on Pix2Vox}.
    In both cases we see a slight increase in \gls{iou} with addition of real data, and a gradual decrease till it reaches 100\% real data}
    \label{fig:mixed1}
\end{figure}

\autoref{fig:mixed2} and \autoref{fig:mixed3}, represent \gls{iou} per category present in the Pix3D and \gls{free}.
The number of images per category is also mentioned on the x-label.
We see that table and desk are most difficult to reconstruct as the \gls{iou} is least, while the wardrobe is the easiest category to reconstruct and gets the most points.
In the majority of the cases, we see that \fls{iou} of models trained on mixed data achieves higher performance than models trained on only real dataset.

In \autoref{fig:mixed_images1}, we see the 3D reconstruction output for models mixed trained with 50\% synthetic and real data per mini-batch.
The outputs were collected for images from the real dataset with the threshold which gave the best \gls{iou}.
The output of models is less noisy than models fine-tuned, as in \autoref{fig:finetuning_images1}, for most reconstructions.
The models seem to have much better details as well.

%\begin{figure}
%    \centering
%    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_linegraph1.pgf}}
%    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_linegraph2.pgf}}
%    \caption{Line plot for the \gls{iou}  for baselines trained on different ratios of synthetic and real dataset.
%        (Left)Mixed training on Pix2Vox++, (right)Mixed training on Pix2Vox. In both cases we see a slight increase in \gls{iou} with addition of real data, and a gradual decrease till it reaches 100\% real data}
%    \label{fig:mixed1}
%\end{figure}

\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_parallel_pix2voxpp.pgf}}
    \caption{Parallel coordinate plot for the \gls{iou} for baseline \texbf{pix2vox++} trained on 50\% of \textbf{mixed dataset}(\gls{s2rv1}, \gls{s2rv2}) and with pix3d.
    The categories are listed along with the number of images.
    The performance of pix2vox++ mixed with both the synthetic dataset is more than model trained on only pix3d, for most of the categories.
    }
    \label{fig:mixed2}
\end{figure}


\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_parallel_pix2vox.pgf}}
    \caption{Parallel coordinate plot for the \gls{iou} for baseline \texbf{pix2vox} trained on 50\% of \textbf{mixed dataset}(\gls{s2rv1}, \gls{s2rv2}) and with pix3d.
    The categories are listed along with the number of images.
    The performance of pix2vox mixed with both the synthetic dataset is more than model trained on only pix3d, for all the categories.}
    \label{fig:mixed3}
\end{figure}


\begin{figure}
    \begin{tabular}{llll}
        Pix3D images & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/baseline/table2}\\

        Ground Truth & \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/bed1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/sofa1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/table2_original}\\

        Output1 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed1_p2vpp_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed1_p2vpp_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed1_p2vpp_table2}\\

        Output2 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed1_p2v_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed1_p2v_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed1_p2v_table2}\\

        Output3 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed2_p2vpp_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed2_p2vpp_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed2_p2vpp_table2}\\

        Output4 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed2_p2v_bed1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed2_p2v_sofa1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/mixed/mixed2_p2v_table2}\\

    \end{tabular}
    \caption{3D reconstruction outputs for best \textbf{mixed training}(50\% per mini-batch) models. Output1-2: Pix2Vox++ and Pix2Vox mixed trained with \gls{s2rv1}.
    Output3-4:Pix2Vox++ and Pix2Vox mixed trained with \gls{s2rv2}}
    \label{fig:mixed_images1}
\end{figure}

\section{Ablation study on chairs}\label{sec:ablation-study-on-chairs}
In this section, we conduct ablation study on chairs by changing domain randomization property.
The dataset used for the study is explained in \autoref{subsec:s2r:3dfree-ablation}, samples of which can be found in \autoref{fig:domain randomisation for ablation study}.
The samples for  each of the datasets used is as shown in \autoref{fig:domain randomisation for ablation study}.
\subsection{Domain randomization on chair dataset}\label{subsec:domain-randomisation-on-chair-dataset}
For comparison with the real dataset, we extract only the chair models from Pix3D and compare the results  with and without 2D augmentation on the synthetic dataset.
In \autoref{fig:ablation1}, we see that the textureless chair dataset outperforms the rest of the dataset.
The hypothesis is that the more domain randomization, the better the performance.

The baseline on real chair dataset from Pix3D is 0.2797 and 0.2916, without and with 2D augmentations on pix2vox++.
For pix2vox we observe 0.2694 and 0.3305 for the same 2 set up.
The textureless chair dataset from \gls{free} gives an \gls{iou} of 0.1798 and 0.143 with light for pix2vox++, 0.0748 and 0.1026 for pix2vox.
From then on, we see a slight increase in \gls{iou} for pix2vox++, but not for pix2vox.
One possible explanation for this behavior can be seen in \autoref{fig:tsne per chair dataset}.
The textureless dataset seems to have more overlap in latent space compared to textureless with light.
In \autoref{fig:ablation1}, we see a slight increase in performance in the 3D reconstruction task, which also holds with \gls{tsne} visualization,
where we see the latent space occupying common regions.
The combined dataset of all other domain randomization showed the best performance of the set for both the baseline models.

\begin{figure}[ht]
    \centering
    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation_barplot1.pgf}}
    \caption{Bar plot for the \gls{iou} for \textbf{baseline} trained on chair dataset with different domain randomization parameters and tested on real dataset.
    We see a dip in performance near textureless dataset, but it gradually increases with addition of domain randomization parameter.}
    \label{fig:ablation1}
\end{figure}

\begin{figure}[!ht]
    \begin{tabular}{llll}
        Pix3D images & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair3}\\

        Ground Truth & \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair2_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair3_original}\\

        Output1 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2vpp_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2vpp_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2vpp_chair3}\\

        Output2 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2v_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2v_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2v_chair3}\\

        Output3 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/ablataion_p2vpp_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/ablataion_p2vpp_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/ablataion_p2vpp_chair3}\\

        Output4 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/ablation_p2v_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/ablation_p2v_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/ablation_p2v_chair3}\\


    \end{tabular}
    \caption{3D reconstruction outputs for best \textbf{ablation} models and models trained on only synthetic dataset. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D.
    Output3-4: Pix2Vox++ and Pix2Vox trained on Multi-object chair synthetic dataset, reconstructs a generic chair with less detail.}
    \label{fig:ablation_images1}
\end{figure}



In \autoref{fig:ablation_images1}, we see the 3D reconstruction output for models trained on only real and only synthetic chair dataset.
The outputs were collected for images from the real dataset with the threshold which gave the best \gls{iou}.
The outputs of models trained on synthetic dataset predict a shape similar to a chair but do not provide fine details as in ground truth.

%\begin{figure}
%    \centering
%    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation_linegraph1.pgf}}
%    \caption{Line plot for the \gls{iou}  for baseline trained on chair dataset with different domain randomization parameters and tested on real dataset.
%    We see a dip in performance near textureless dataset, but it gradually increases with addition of domain randomization parameter.}
%    \label{fig:ablation1}
%\end{figure}


\subsection{Domain randomization with Mixed training}\label{subsec:domain-randomisation-with-mixed-training}

To reiterate,the performance of Pix2Vox++ on real chair dataset(Pix3D) is 0.2797 and 0.3305, without and with 2D augmentations.
For Pix2Vox, it was 0.2694 and 0.2916, with and without 2D augmentation, respectively.
In mixed training with a ratio of 50\%, we see a maximum increase of 3.43\% increment in pix2vox++ and 5.15\% in pix2vox.
The behavior of models for the different randomization parameters is similar to what we observed in \autoref{subsec:domain-randomisation-on-chair-dataset}.
For pix2vox++, the textureless chair dataset gives the best performance, with a gradual decrease with the addition of each parameter and a slight increase for the multi-object dataset.
Similarly, for pix2vox, a gradual decrease is observed, but multi-object gives better performance than the textureless dataset, as in \autoref{fig:ablation2}.
The combined dataset of all other domain randomization showed the best performance of the set for both the baseline models.

We initially expected to see the performance same as in \autoref{fig:ablation1}, with each component of domain randomization.
However, Mixed training eliminated the inconsistency, and thus irrespective of the type domain randomization in synthetic data, the validation achieves good consistent performance as in \autoref{fig:ablation2}.

\begin{figure}[ht]
    \centering
    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation_barplot2.pgf}}
    \caption{Bar plot for the \gls{iou}  for baseline trained by \textbf{mixing} chair dataset from real and synthetic dataset with ratio of 50\%.
    Observe that the \fls{IoU} is consistent for all types of randomization proving that mixed training negates loss from randomization.]}
    \label{fig:ablation2}
\end{figure}


\begin{figure}[!ht]
    \begin{tabular}{llll}
        Pix3D images & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair3}\\

        Ground Truth & \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair1_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair2_original} &
        \includegraphics[trim={0 0 {.1\width} 0},clip,width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/chair3_original}\\

        Output1 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2vpp_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2vpp_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2vpp_chair3}\\

        Output2 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2v_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2v_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/pix3d_p2v_chair3}\\

        Output3 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/mixed_p2vpp_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/mixed_p2vpp_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/mixed_p2vpp_chair3}\\

        Output4 & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/mixed_p2v_chair1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/mixed_p2v_chair2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/ablation/mixed_p2v_chair3}\\

    \end{tabular}
    \caption{3D reconstruction outputs for best ablation models and models with \textbf{mixed} training. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D.
    Output3-4:Pix2Vox++ and Pix2Vox trained with mixed ratio of 50\% with multi-object chair synthetic dataset, reconstructs chair with more details.}
    \label{fig:mixed_ablation_images2}
\end{figure}

In \autoref{fig:mixed_ablation_images2}, we see the 3D reconstruction output for models trained on mixed dataset of 50\% of synthetic and real per mini-batch.
The outputs were collected for images from the real dataset with the threshold which gave the best \gls{iou}.
The output is better than models trained on only synthetic dataset as in \autoref{fig:ablation_images1}.
The detailing in the reconstructed chair even seems to be better than the models trained on only the real dataset in the same image.

%\begin{figure}
%    \centering
%    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation_linegraph2.pgf}}
%    \caption{Line plot for the \gls{iou}  for baseline trained by mixing chair dataset from real and synthetic dataset with ratio of 50\%.
%    Observe that the \fls{IoU} is consistent for all types of randomization proving that mixed training negates loss from randomization.]}
%    \label{fig:ablation2}
%\end{figure}


\section{Discussion}\label{subsec:discussion}

Pix3D is a dataset that contains images from the real world as discussed in \autoref{sec:pix3d}.
However, these images are not just from the real environment.
Some images are taken from advertisements with text on the image, while others are without any background textures.
The furniture under observation may not even be in the focus of the image.

We train the models trained on synthetic \gls{free} datasets and evaluate them on real datasets to randomize most generic environments.
The ground truth has only 1 furniture model as output per image.
However, we also see a model predicting more than one piece of furniture as output when more than one types of furniture are in focus as in \autoref{fig:interesting1}.
The model seems to be learning the context of furniture placement without us feeding any additional information.

Another scenario where the model predicts something other than ground truth is when the furniture under focus is different.
In \autoref{fig:interesting2},, the furniture under observation is the desk. However, the model chose to focus on the chair in front of it to reconstruct.
These are some interesting findings inference for which can be the future scope of this thesis.

\begin{figure}[ht]
    \begin{tabular}{lll}
        \includegraphics[width=.3\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/interesting/p2ppp_table_input} &
        \includegraphics[width=.3\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/interesting/p2ppp_table1_original}&
        \includegraphics[width=.3\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/interesting/p2ppp_table1_output}\\
    \end{tabular}
    \caption{A sample output where more than one furniture seem to be predicted.
            (left) Input image, (center) Ground truth, (right) Predicted output.
    The model seems to predict both the chair and the table with the context it learnt during training.}
    \label{fig:interesting1}
\end{figure}


\begin{figure}[ht]
    \begin{tabular}{lll}
        \includegraphics[width=.3\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/interesting/p2ppp_table2_input} &
        \includegraphics[width=.3\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/interesting/p2ppp_table1_original}&
        \includegraphics[width=.3\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/reconstruction/interesting/p2ppp_table2_output}\\
    \end{tabular}
    \caption{A sample output where a more focused furniture is getting predicted instead of ground truth.
        (left) Input image, (center) Ground truth, (right) Predicted output.    The model seems to predict the chair instead of the table.
    }
    \label{fig:interesting2}
\end{figure}
