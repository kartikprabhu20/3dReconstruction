\chapter{\iftoggle{german}{Evaluierung}{Experiments and Evaluation}}\label{ch:evaluation}

%\todo{
%    \begin{enumerate}
%        \item baseline comparison, pix3d pix2vox, pix2vox++ classwise)
%        \item values per category
%        \item baseline different version of dataset with different threshold
%        \item ablation values per model
%        \item !!!!!finetuning with different datasize (pending)
%        \item !!!!!mixed training with different real datasize(pending)
%        \item output diagrams
%        \item training graphs
%    \end{enumerate}
%}

In this chapter we will conduct experiments that will help us find solutions for the research questions discussed in~\ref{sec:goal}.
Section~\ref{sec:a-survey-on-photorealism} will contain the survey results conducted to check the photorealism of the proposed synthetic dataset.
In this section we will also compare the ratings given to other proclaimed photorealistic dataset and check whether \gls{free} dataset compares to those datasets.
We will further evaluate the datasets using a T-SNE as qualitative measure and MSE \& FID as quantitative measure which will indicate the domain gaps with respect to the real dataset.

In section~\ref{sec:datasets}, we describe different datasets specifically generated to evaluate our baseline models and check randomisation parameters.
Section~\ref{sec:baseline} evaluates baseline models with real and synthetic versions of datasets.
Section~\ref{sec:fine-tuning} further evaluates models pretrained on synthetic dataset by fine-tuning them using real dataset.
In section~\ref{sec:ablation-study-on-chairs} we will evaluate models on chair dataset with different parameters of randomisation and also mixed training for these individual datasets.

\section{A survey on photorealism}\label{sec:a-survey-on-photorealism}
The participants were given minimalistic information about the intention behind the survey.
The goal of the survey was to analyse if humans have the same perception for photographic and computer generated images.
No time limit was set for the survey, and it was open to everyone.
A total of 72 participants responded to the survey.
The survey was created using Google forms, and the link was distributed.
The participants either used a mobile phone or a desktop to respond to the survey.
A total of 9 datasets were used in the survey.
\gls{front}~\cite{Fu20203DFRONT3F}, Hypersim~\cite{Roberts2020HypersimAP}, InteriorNet~\cite{InteriorNet18}, SceneNet~\cite{McCormac:etal:ICCV2017}, BlenderProc~\cite{denninger2019blenderproc},
\gls{ai2thor}~\cite{kolve2019ai2thor}, Openroom~\cite{li2021openrooms}, Pix3D~\cite{pix3d} and proposed \gls{free} dataset.
Only Pix3D was a real dataset while all others are synthetic dataset proclaimed to be photorealistic.

The survey was composed of 3 sections.
\begin{enumerate}
    \item Section 1: Decide if the image is real or not real.

    In this section there were a total of 27 images, 3 each from the above mentioned datasets.
    Each image had only 2 options to select: "Real" or "Not real"
    This approach eliminated any ambiguous perception towards the images.

    \item Section 2: Rate the image on scale of 1 to 10 in terms of realism (1 -> least real, 10 -> most real).

    In this section, the participant used a likert scale to rate the images based on photorealism.
    Similar to section 1, there were 27 questions of 3 images per dataset.

    \item Section 3: Rank the images from 1 to 9 (1 -> Most real, 9 -> Least real).

    In this section, the participant had only 3 questions, with each question having an image from each of the dataset arranged in a 3\x3 grid format.
    The users were asked to rank them in the increasing order of the photorealism.
\end{enumerate}

\subsection{Survey results}\label{subsec:survey-results}
In this segment, we discuss the results of survey collected from participants.

\subsubsection{Section 1: Real or Not}
In section 1, the participants had only 2 options to select from independently.
Figure~\ref{fig:question1}, shows that the real dataset Pix3D~\cite{pix3d} was rightly recognised as real.
77\% of the real images that belonged to Pix3D were recognised to be real.
This shows that the participants were not convinced even with the real images as 23\% of the images were still recognized as not real.
Among the synthetic datasets, Hyeperism~\cite{Roberts2020HypersimAP} got the best results of 59.7\% identified as real.
\gls{ai2thor} had the least amount of images recognised as real with just 5\% positive responses.
The proposed \gls{free} dataset had 8\% of images identified as real.
Both of these datasets were built Unity, which shows that images generated using the automated Unity framework needs some improvement.
Suppose we have a threshold of 50\%, we see that the datasets for which the images selected as Not real below the threshold value belong to datasets which are automated and not created by professionals manually.
As mentioned in~\ref{subsec:indoor-synthetic-datasets}, Openrooms, SceneNet and Blenderproc are datasets obtained from automation.
We consider \gls{free} dataset to be automated and belog to this category.
Among the automated images, Openrooms have got most vote of confidence with 37\% recognised as real images.
Even though \gls{free} was least recognised as real among the automated tools, it had better percentage than AI2THOR which has Unity based framework to generate images and was manually configured by professional by taking in reference of real world images.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question1.pgf}}
    \caption{The figure represents distribution for Section 1 of survey. The participants were asked to distinguish if the image was 'Real' or 'Nor Real'.
    The automated dataset is highlighted with bolder color. The dotted line is 50\% threshold. All the automated dataset have less than threshold votes for 'Real'.}
    \label{fig:question1}
\end{figure}

\subsubsection{Section 2: Likert Scale}
In section 2, the participants could select ratings from 1 to 10 (1 being the least photorealistic).
The distribution of values for each dataset can be seen in figure~\ref{fig:question2} and the average ratings are as seen in figure~\ref{fig:question2_2}.
If we consider the scale 1, which is least rating that can be given to the image, AI2THOR has most number of votes.
Suppose we have a cut off at scale 2 and 3, Openrooms and \gls{free} are the least photorealistic respectively.
Interestingly Interiornet has least number of scale 1 instead of Pix3D which is the real dataset.
However, Pix3D has the highest number of perfect score(10) among all the datasets.
Coming to the averages, we again see the datasets created from automated pipeline (Blenderproc, SceneNet, Openrooms,\gls{free}), have least average ratings,
while the manually created datasets(Hyperism, \gls{front}, InteriorNet) have higher average ratings.
Pix3D has highest of the average ratings closely followed by InteriorNet.
Even though \gls{free} has the least average, it is still comparable to other automated pipelines as highlighted in the figure~\ref{fig:question2_2} and even Unity based AI2THOR dataset.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question2.pgf}}
    \caption{The figure represents distribution for Section 2 of survey. The participants were asked to rate the image based on photorealism(1 being the least photorealistic).
    \gls{free} has maximum number of least rating(1), but it is comparable to other automated datasets as highlighted. Pix3D has maximum number of highest rating(10).}
    \label{fig:question2}
\end{figure}

\begin{figure}
    \centering
    \resizebox{\textwidth}{10cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question2_2.pgf}}
    \caption{The figure represents average rating given by the participant to each of the datasets in section 2 of the survey.
    The automated datasets are highlighted, all of them have lower average. \gls{ai2thor}(Unity based and manually designed) is also among the lower averages.}
    \label{fig:question2_2}
\end{figure}

\subsubsection{Section 3: Rank by comparison}
In section 3 of the survey, the users compared all 9 datasets and ranked them according to their photorealism(1 being the best rank).
Figures~\ref{fig:question3} and~\ref{fig:question3_2} show distribution and average ranking for each of the datasets.
The real dataset Pix3D got the highest number of votes for rank 1, while \gls{free} got least.
But if we have a threshold of 5, meaning frequency of votes being in top 5 ranks, then among the automated group, \gls{free} occurs most times followed by SceneNet, Openrooms and Blenderproc.
This is significant because out of 9 datasets, 4 are automated, and these four have the least average rankings.
But \gls{free} breaks the boundary to be in top 5 most times.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question3.pgf}}
    \caption{The figure represents distribution for Section 3 of survey. The participants were asked to rank the images based on photorealism(1 being the best) by comparing images from all 9 datasets.
    The automated datasets are highlighted. \gls{free} dataset appears in top 5 maximum number of times, seen to the left of the dotted line.}
    \label{fig:question3}
\end{figure}

\begin{figure}
    \centering
    \resizebox{0.75\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/survey/question3_3.pgf}}
    \caption{The figure represents box plot for section 3. The orange horizontal line within the box indicates median, the blue dotted line indicates mean.
    The bolder boxes represents automated dataset, while lighter boxes represent manually created dataset. \Gls{free} has highest mean among the automated datasets.}
    \label{fig:question3_2}
\end{figure}

\subsubsection{Summary for survey}
In the survey we observe that Pix3D was rightly chosen as photorealistic image as they are collected from real world data.
Though the proposed \gls{free} dataset falls behind in comparison to manually designed images from Hyperism, InteriorNet and \gls{front},
it is comparable to automated datasets from Blenderproc, OpenRooms and SceneNet.
It also trumps over AI2THOR which is manually designed using Unity game engines.

\section{Domain gaps}\label{sec:domain-gaps}

In this section, we verify if the synthetic dataset;
\gls{free}, has domain gap with the real dataset(Pix3D).
Along with the new synthetic dataset, we will compare the datasets used for the survey in section ~\ref{sec:a-survey-on-photorealism}.
Qualitatively we will visualise dataset embeddings using T-SNE in ~\ref{subsec:qualitative}, and quantitatively we will compare the distributions of all the synthetic dataset and real dataset~\ref{subsec:quantitative}.

\subsection{Qualitative}\label{subsec:qualitative}

For qualitative assessment of the domains for each of the dataset, we utilize T-SNE visualisations of embedding space from \gls{vgg}~\cite{simonyan2015deep}.
We consider a \gls{vgg}16 model pretrained on ImageNet~\cite{Deng2009ImageNetAL} and use it as an encoder to embed the image space of all the images from each of the datasets.
This latent space embedding is then converted to 2-Dimensional representation using T-SNE visualisation.
A model trained on ImageNet can be used for encoding the images since it contains all the furniture categories present in Pix3D\@.
And hence the images will be respectfully embedded and mapped to 2D space.

For each dataset, 30 images were randomly chosen and passed through the encoder at the same time.
The images are less in quantity because not all datasets provide images directly.
Some datasets like Hyperism, SceneNet, Openrooms are built for training SLAM(Simultaneous Localisation and Mapping) models, and thus not all frames contain furniture.
We had to filter the images containing furniture so that time images appear to be in same embedding space.

In figure ~\ref{fig:photorealistic tsne}, we see the embedding for each dataset used for the survey discussed in~\ref{sec:a-survey-on-photorealism}.
For better visualisation, the points are connected to centroid of the dataset,
if not we found it difficult to comprehend scatterplot of all datasets in single graph.
The proposed \gls{free} and the real dataset;Pix3D are highlighted for better focus.

From the T-SNE visualisation, we see that the real dataset(Pix3D) is spread across the space and lies at the centre of the plot.
\gls{ai2thor}, openrooms, hyperism and \gls{front} have their embedding mapped in the outer region.
BlenderProc and SceneNet seem to be closest to the real dataset, but the latent space is not widespread indicating lesser randomisation.
\gls{free} has a wide spread and is closer to the real dataset.
This can also be seen in figure~\ref{fig:pix3d_s2r3dfree}.
For this visualisation, 280 randomly images were randomly selected from each dataset with 40 images belonging to each of the categories.

We plot the visualisation for each dataset individually with real dataset in figure~\ref{fig:tsne per dataset}.
From this figure we can clearly observe that the latent space of real dataset and \gls{free} are very close to each other and spread across the space.
We can also see that \gls{ai2thor} has the maximum separation, while Blenderproc is as close as \gls{free}.
Openrooms and SceneNet also have latent space significantly apart from real dataset, while Hyperism and \gls{front} have a widespread space that intersects real data.

\begin{figure}
    \centering
    \resizebox{\textwidth}{10cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/photorealisitic_dataset_2.pgf}}
    \caption{T-SNE visualisation for images from various photo-realistic synthetic dataset. Pix3d and \gls{free} are highlighted with bolder colors.
    Both these datasets have wide spread in the embedding space.}
    \label{fig:photorealistic tsne}
\end{figure}

\begin{figure}
    \centering
    \resizebox{\textwidth}{10cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/s2r3dfree_Pix3d280.pgf}}
    \caption{T-SNE visualisation for images from Pix3d and \gls{free} dataset.We observe that \gls{free} still doesnt encapsulate the embedding space like Pix3D.}
    \label{fig:pix3d_s2r3dfree}
\end{figure}

\begin{figure}[!ht]
    \centering
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/openrooms_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/scenenet_Pix3d.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/blenderproc_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/ai2thor_Pix3d.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/3dfront_Pix3d.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/hypersim_Pix3d.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/s2r-3dfree_Pix3d.pgf}}\\
%    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/interiornet_Pix3d.pgf}}

    \caption{T-SNE visualisation for images from individual photo-realistic synthetic dataset compared with Pix3D latent space.
        (Left to right, top to bottom) Openrooms, SceneNet, Blenderproc, \gls{ai2thor}, \gls{front}, Hyperism, \gls{free} in blue;
        compared with Pix3D in orange.}
    \label{fig:tsne per dataset}
\end{figure}


\subsection{Quantitative}\label{subsec:quantitative}

We visualised the embedding space using T-SNE in the above subsection~\ref{subsec:qualitative}.
All the datasets seem to have a very close relation with the real dataset as we see atleast some points being in intersection with the latent space of real dataset.
In this section,we compare the datasets with quantitative assessment using Mean Squared Error(MSE) and Fr\'echet Inception Distance(FID)
As seen in the table~\ref{tab:quantitative-dataset-comparison}, we see that BlenderProc has least MSE of 5.53 to Pix3D,
while openrooms has highest MES of 7.63.
\gls{free} has a MSE of 7.075 which is below Openrooms and Hyperism.
Interestingly, \gls{ai2thor} seems to perform better in quantitative assessment of photorealism,
where both MSE and \gls{fid} are considerably lesser than other dataset,
proving that \gls{ai2thor} is closer to real dataset than what was visualised in figures~\ref{fig:photorealistic tsne} and~\ref{fig:tsne per dataset}.
\gls{free} has a \gls{fid} of 178.83 which is lesser than Openrooms, Hyperism and SceneNet.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c |c |c |c|}
        \hline
        Dataset & \gls{mse} & \gls{fid} \\ [0.5ex]
        \hline\hline
        Openrooms & 7.63 & 189.43 \\
        \hline
        \gls{ai2thor} & 6.94 & 164.61 \\
        \hline
        BlenderProc & 5.53 & 173.37 \\
        \hline
        Hyperism & 7.12 & 186.57 \\
        \hline
        \gls{front} & 6.93 & 167.65 \\
        \hline
        InteriorNet & xx & xx \\
        \hline
        SceneNet & 6.7553 & 185.49 \\
        \hline
        \gls{free} & 7.0750 & 178.83 \\[1ex]
        \hline
    \end{tabular}
    \caption{Table represents quantitative measure to compare synthetic dataset distribution with the real dataset(Pix3D)}
    \label{tab:quantitative-dataset-comparison}
\end{table}

\section{Datasets}\label{sec:datasets}
In this section, the datasets used for the following evaluations will be described.
The datasets are intended to have variations in domain randomisation to check its performance on 3D reconstruction task.

\subsection{Pix3D}
As mentioned in~\ref{subsec:why-pix3d?}, we use a real dataset from~\cite{pix3d} which is a collection of indoor scenes.
The 2 classes 'misc' and 'tools' are eliminated so that we focus only on furnitures.
The total images after the reduction is 9954 with 354 unique models.
The train and validation dataset is divided in the ratio of 70:30 giving us 6814 images from training and 3140 images for validation/test.
We do not have a test set only for this dataset since it is already limited, and the validation set is used as test set while testing with synthetic data.
Samples are as shown in~\ref{fig:samples for synthetic and real comparison}.

\subsection{\gls{free} Version 1}
Version 1 of \gls{free} was created by keeping the models in the center of a default 3D room.
The camera distance was randomised between 1 and 2.5 meters from the model.
The camera view points and textures were randomised.
A total of 70000 images were synthetically generated using the \gls{free} 'Single Room pipeline' with 10000 images per category.
Samples are as shown in~\ref{fig:samples for synthetic and real comparison}.

\subsection{\gls{free} Version 2}
Version 1 of \gls{free} was created by keeping the models in the center of a defualt 3D room.
The camera distance was randomised between 1 and 2.5 meters from the model.
The camera view points and textures were randomised.
A total of 21000 images were synthetically generated using the \gls{free} 'Multi Object pipeline' with 3000 images per category.
Samples are as shown in~\ref{fig:samples for synthetic and real comparison}.

\subsection{\gls{free} Ablation}\label{subsec:s2r:3dfree-ablation}
To study the affects of parameters of domain randomisation, a study was conducted on chair dataset by omitting few factors one at a time.
This dataset were divided into 5 categories with different randomisation parameters.
The sample images with different randomisation is as shown in figure~\ref{fig:domain randomisation for ablation study}.
Figure~\ref{fig:tsne per chair dataset} represents latent space for each of the variation of domain randomisation for chairs when compared to chairs from real dataset(Pix3D).
50 images were randomly sampled from each dataset and embeded using \gls{vgg} as in~\ref{subsec:qualitative}.
Figure~\ref{fig:pix3dchair_s2r3dfreechair} shows a combined latent space of all the randomised dataset for chair.
We see that combined space is close to the real dataset space and spreads across the latent space, while individual parameters have limited spread.o

\begin{figure}[!ht]
    \centering
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_textureless.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_textureless_light.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_background.pgf}}
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_background_light2.pgf}}\\
    \resizebox{0.49\linewidth}{5cm}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_s2r3dfree_chair.pgf}}
    \caption{T-SNE visualisation for images from individual synthetic chair dataset with different domain randomisation parameter compared with Pix3D chair latent space.
        (Left to right, top to bottom) Textureless, Textureless with light, Textured, Textured with light, Multi-Object}
    \label{fig:tsne per chair dataset}
\end{figure}


\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/domaingap/Pix3d_chair_DR.pgf}}
    \caption{A combined T-SNE visualisation for chair images from Pix3d and \gls{free} dataset with different parameters for randomisation.}
    \label{fig:pix3dchair_s2r3dfreechair}
\end{figure}

\subsubsection{2R:3DFREE\_Textureless}

    For textureless dataset, the chair models were kept at the center of an un-textured room with constant light source.
    A total of 10000 images were generated from different camera viewpoints.

\subsubsection{\gls{free}\_Textureless\_Light}

    For this dataset, similar to the textureless dataset the chair models were kept at the center of an un-textured room, but with randomised light source.
    The light variation was implemented as in~\ref{subsec:lightings-and-shadows}.
    Along with randomised light source, the camera view points were randomised with a distance in range of 0.75 to 1.5 meters from the model under observation.

\subsubsection{\Gls{free}\_Textured}

    As the name suggests, in this dataset both the model and the default single room were textured randomly for each snapshot as explained in~\ref{subsec:randomised-texture}.
    10000 snapshot of chair models were taken using different camera viewpoints.

\subsubsection{\Gls{free}\_Textured\_Light}

    This is an extension of the above mention \gls{free}\_Textured dataset, with addition of randomized light sources.
    The lights are randomized as implemented in~\ref{subsec:lightings-and-shadows}.

\subsubsection{\gls{free}\_Chair}

    \Gls{free}\_Chair dataset was created using 'Multi Object pipeline' with chair replacing a similar category from scene as implemented in~\ref{subsec:replacing-target-objects}.
    Both the light and camera view points were randomised making sure that the model under observation is not completly occluded.


\begin{figure}
    \begin{tabular}{llll}
        Pix3D & \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_1} &
        \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_2} &
        \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/pix3d_3}\\

        \Gls{free} Version 1 & \includegraphics[width=.2\textwidth, height =.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_1} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_2} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v1_3}\\

        \Gls{free} Version 2 & \includegraphics[width=.19\textwidth, height =.2\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_1} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_2} &
        \includegraphics[width=.2\textwidth, height=.19\textwidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_v3_3}\\

    \end{tabular}
    \caption{Samples of images from real and synthetic datasets.}
    \label{fig:samples for synthetic and real comparison}
\end{figure}


\begin{figure}
    \begin{tabular}{llll}
        Textureless & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_3}\\

        Textureless with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textureless_light_3}\\

        Textured & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_3}\\

        Textured with light & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_textured_light_4}\\

        Multi-Object & \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_1} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_2} &
        \includegraphics[width=.2\linewidth]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/datasets/s2r_chair_3}\\

    \end{tabular}
    \caption{Samples of images used for ablation study on chairs with different parameters of domain randomisation.}
    \label{fig:domain randomisation for ablation study}
\end{figure}

\section{Baseline}\label{sec:baseline}

As mentioned in~\ref{subsec:pix2vox-and-pix2vox++}, pix2vox and pix2vox++ are the models which will act as the baselines for all the experiments.
For the dataset from~\ref{sec:datasets}, Pix3D is the real dataset and will be acting as the base dataset.
The models are also compared with and without 2D augmentation.
The 2D augmentations include Random Flip, Random Crop, Color Jitter, RandomPermuteRGB.
A performance test with no augmentation will explain the importance of 2D augmentation in the performance of 3D reconstruction task.
Figure~\ref{fig:baseline1}, represents performance of baseline models on different datasets, both with and without data augmentation.
For validation step, we save 2 checkpoints for the best epoch .
One which is validated with real dataset, and the other is validated with respective synthetic dataset.

For the synthetic dataset, the models are trained on 70\% of data and 30\% is used for validation.
For testing we use the same 30\% of real data from Pix3D dataset.
For the real dataset, the models are trained on 70\% of real data, and validated/tested on same 30\% used in testing models trained on synthetic data.

For models validated with real dataset, it is seen that 2D augmentation improves \gls{iou}  by 8.06\% for Pix3D on pix2vox++, and 1.22\% on pix2vox.
In case of synthetic dataset, 2D augmentation increased the \gls{iou}  by 2.04\% and 4.77\%  for pix2vo++ and pix2vox respectively.
When it comes to whether synthetic dataset gives equivalent performance as real dataset, we can clearly see a dip in the performance,
proving that there is a domain gap between real and synthetic data.

\begin{figure}
    \centering
    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline_linegraph1.pgf}}
    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/baseline_linegraph2.pgf}}
    \caption{Line plot for the \gls{iou}  for baselines trained on real and synthetic datasets, with and without 2D augmentation.
        (Left)The checkpoint was saved using real dataset for validation step, (right) the checkpoint was saved using corresponding synthetic data for validation step.
        In both the cases we see that ~\gls{free} does not perform adequately on its own. \gls{s2rv2} contributes better than \gls{s2rv1}.}
    \label{fig:baseline1}
\end{figure}

%\todo{replace value of s2r\_v1(currently values are from s2r\_v4)}

\section{Fine Tuning}\label{sec:fine-tuning}
Fine tuning or Transfer Learning is a common way of domain adaptation.
For this experiment, the model is first trained on synthetic dataset and then used as a pre-trained model to be fine tuned using real dataset.

In figure~\ref{fig:finetuning1}, we have a comparison of \gls{iou}  with pure real and pure synthetic dataset, followed by fine-tuning the models with real dataset.
The core comparison is between real data and fine-tuned model.
Models are pre-trained with 2 versions of \gls{free}(\gls{s2rv1} and \gls{s2rv2}) as mentioned in~\ref{sec:datasets}.
It is noticed that for \gls{s2rv1} there is a decrement of 3.04\% and 1.25\% on pix2vox++ and pix2vox respectively.
For \gls{s2rv2}, a decrement of 3.54\% on pix2vox++, but an increment of 0.8\% on pix2vox model.

\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/finetuning_linegraph1.pgf}}
    \caption{Line plot for the \gls{iou} for baseline models(Pix2Vox++ and Pix2Vox) trained on synthetic and fine-tuned with real dataset.
    We see that even after fine-tuning both the models do not perform as good as models trained on only real dataset.}
    \label{fig:finetuning1}
\end{figure}

\section{Mixed Training}\label{sec:mixed-training}
For mixed training, we mix synthetic and real dataset with a fixed ratio in each of the mini-batches.
The ratios used ware \emph{0.15, 0.25, 0.5, 0.75 and 0.9}.
Higher the ratio, closer the mixed dataset becomes a real dataset.
Both real and synthetic dataset drawn according to these ratios for each mini-batch.
As synthetic dataset is much more abundant than real, the real dataset will be oversampled to achieve the mentioned ratios.

In figure~\ref{fig:mixed1}, with each of the ratios the performance is better than the baseline of 0.3443 for pix2vox++,
except for a ratio 15\%, which in some cases has slightly lower \gls{iou}  value.
Most significant difference is seen for at 50\% using \gls{s2rv2} dataset with an increase of 3.04\%.
We saw an increase in \gls{iou} for pix2vox rather than pix2vox++.
An increase of 2.62\% is noted for 25\% mix with \gls{s2rv2} on pix2vox.

In the figure~\ref{fig:mixed2}, we represent \gls{iou} per category present in the Pix3D and \gls{free}.
The number of images per category is also mentioned on the x-label.
We see that table and desk are most difficult to reconstruct as the \gls{iou} is least, while the wardrobe is the easiest category to reconstruct and gets the most points.
In the majority of the cases we see that \fls{iou} of models trained on mixed data achieves higher performance than models trained on only real dataset.

\begin{figure}
    \centering
    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_linegraph1.pgf}}
    \resizebox{0.49\linewidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_linegraph2.pgf}}
    \caption{Line plot for the \gls{iou}  for baselines trained on different ratios of synthetic and real dataset.
        (Left)Mixed training on Pix2Vox++, (right)Mixed training on Pix2Vox. In both cases we see a slight increase in \gls{iou} with addition of real data, and a gradual decrease till it reaches 100\% real data}
    \label{fig:mixed1}
\end{figure}

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/mixed_parallel_pix2vox.pgf}}
    \caption{Parallel coordinate plot for the \gls{iou} for baseline pix2vox trained on 50\% of mixed dataset(\gls{s2rv1}, \gls{s2rv2}) and with pix3d.
    The categories are listed along with the number of images.
    The performance of pix2vox mixed with both the synthetic dataset is more than model trained on only pix3d, for all the categories.}
    \label{fig:mixed2}
\end{figure}

\section{Ablation study on chairs}\label{sec:ablation-study-on-chairs}
In this section, we conduct ablation study on chairs by changing domain randomisation property.
The dataset used for the study is explained in subsection~\ref{subsec:s2r:3dfree-ablation}.
The samples for  each of the dataset used is as shown in~\ref{fig:domain randomisation for ablation study}.

\subsection{Domain randomisation on chair dataset}\label{subsec:domain-randomisation-on-chair-dataset}
For comparison with real dataset, we extract only the chair models from Pix3D and compare the results  with and without 2D augmentation on the synthetic dataset.
In figure~\ref{fig:ablation1}, we see that textureless chair dataset out performs rest of the dataset.
Contrary to the hypothesis that more the randomisation, better the performance, we see that as the randomisation is increased the performance has decreased.

The baseline on real chair dataset from Pix3D is 0.2797 and 0.3305, without and with 2D augmentations on pix2vox++.
For pix2vox we observe 0.2694 and 0.2918 for the same 2 set up.
The textureless chair dataset from \gls{free} gives an \gls{iou} of 0.1798 and 0.143 with light for pix2vox++, 0.0748 and 0.1026 for pix2vox.
From then on we see slight increase in \gls{iou} for pix2vox++, but not for pix2vox.

\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation_linegraph1.pgf}}
    \caption{Line plot for the \gls{iou}  for baseline trained on chair dataset with different domain randomization parameters and tested on real dataset.
    We see a dip in performance near textureless dataset, but it gradually increases with addition of domain randomization parameter.}
    \label{fig:ablation1}
\end{figure}

\subsection{Domain randomization with Mixed training}\label{subsec:domain-randomisation-with-mixed-training}

To reiterate,the performance on real chair dataset from Pix3D is 0.2918 and 0.3308, 0.2664 and 0.2907 without and with 2D augmentations on pix2vox++ and pix2vox respectively.
In the mixed training with a ratio of 50\%, we see a maximum increase of 3.4\% increment in pix2vox++ and 5.24\% in pix2vox.
The behavior of models for the different randomisation parameters is similar to what we observed in ~\ref{subsec:domain-randomisation-on-chair-dataset}.
For pix2vox++, the textureless chair dataset gives the best performance, with gradual decrease with addition of each parameter and slight increase for multi-object dataset.
Similarly, for pix2vox the gradual decrease is observed, but multi-object gives better performance than the textureless dataset.
These observations are seen in figure~\ref{fig:ablation2}.

Hypothesis would be that with each percentage of real data added, the final \gls{iou} changes in proportion to the values in ~\ref{fig:ablation1}; that is we would expect a similar shaped curved with better \gls{iou} values.
But we observe that mixed training eliminated this inconsistency, and thus irrespective of the type domain randomisation in synthetic data, the validation achieves good consistent performance.
Hence, we see a flat line in figure~\ref{fig:ablation2}.
\begin{figure}
    \centering
    \resizebox{0.7\textwidth}{!}{\input{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/evaluation/performance/ablation_linegraph2.pgf}}
    \caption{Line plot for the \gls{iou}  for baseline trained by mixing chair dataset from real and synthetic dataset with ratio of 50\%.
    Observe that the \fls{IoU} is consistent for all types of randomization proving that mixed training negates loss from randomization.]}
    \label{fig:ablation2}
\end{figure}
