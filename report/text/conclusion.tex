\chapter{\iftoggle{german}{Fazit}{Conclusion}}\label{ch:conclusion}

This chapter summarizes(\autoref{sec:Summary}) the entire thesis and answers the research questions discussed in \autoref{sec:goal}.
We will further discuss the limitation of the work indulged in this thesis in \auto{sec:limitations-of-game-engines2}.
We will also mention possible future scopes and ideas which we came across while conducting this thesis.


\section{\iftoggle{german}{Zukünftige Arbeiten}{Summary}}\label{sec:Summary}

In \autoref{ch:introduction}, we describe the main aim of the thesis to create a synthetic dataset that can be used for 3D reconstruction tasks for furniture as there is a minimal number of datasets open to researchers.
We also explained why synthetic datasets are essential and how they will be handy for future tasks.
We also introduced game engines and how we intend to use the Unity framework in our pipeline.

In \autoref{ch:related_work}, we provided an overview of existing proclaimed photorealistic datasets along with their use cases.
These datasets were further used in the survey to verify the photorealism from the human perspective;
the images can be seen in \autoref{fig:photorealistic images comparison}.
The tools to create synthetic datasets were discussed in \autoref{subsec:tools-to-create-synthetic}, wherein we discussed the underlying framework used to build the tool.
Then we came across deep learning techniques and models used for 3D reconstruction with different representations.
In the same section, we discussed some similar works wherein synthetic datasets were used to enhance the performance of real datasets by techniques like fine-tuning and mixed training.

\autoref{ch:concept} discussed the benchmark datasets Pix3D~\cite{Sun2018} and SceneNet~\cite{McCormac2017} and how we use them in our synthetic dataset generation pipeline.
We then discussed the role of the Unity game engine as a framework for different types of domain randomization and in the generation of the synthetic dataset.
Then came the introduction of the \gls{free} dataset and its distribution.
We then moved to the thesis's Deep Learning aspect and discussed baseline models Pix2Vox and Pix2Vox++ used for the 3D reconstruction task.

We used the Unity game engine for implementation, and the application details were provided in \autoref{ch:implementation}.
Using different modes of operation and importing models from Pix3D and SceneNet, the pipeline can create an ersatz environment for our dataset.
We then discussed domain randomization achieved from camera viewpoints, textures, lighting, and shadows and replacing category objects.
The snapshots were taken using Unity technologies' library called ML-ImageSynthesis, which generates 2.5D images, i.e., Depth, normals, flows, segmentation. Then we see the Deep Learning framework and its configurations.
Then we see the Deep Learning framework and its configurations.

We dedicate \autoref{ch:evaluation} for experiments and evaluation.
We evaluate the photorealism of datasets compared to the real dataset(Pix3D) from a survey.
The survey answers our first research question: \textbf{\emph{Are game engines a reliable medium for creating a photorealistic synthetic dataset?}}
The survey results discussed in \autoref{subsec:survey-results} concluded that the proposed dataset using unity game engine,
although comparable with other automated dataset collections,
still lags the photorealism from real datasets or even the manually created datasets by architects.
The \gls{free} dataset outperformed some synthetic datasets regarding the individual rating but was in the top 5 out of 9 datasets while conducting a comparative study.

For our second research question: \textbf{\emph{Can Ersatz environment from a game engine like Unity replace real data for training in 3d reconstruction tasks?}}
We trained baseline models for the 3D reconstruction task with real and synthetic datasets and tested them on a real dataset.
In \autoref{sec:baseline}, we see a drop of around 14\% in performance when we use only the synthetic dataset.
Hence, we conclude that the \gls{free} synthetic dataset is not reliable as a standalone dataset.

Our third research question was: \textbf{\emph{Can we improve the performance of a model using a blend of real and synthetic datasets?
To what extent does the performance enhance when the synthetic dataset is ten times that of a real dataset?}}.
To answer this, we implemented domain adaptation techniques.
When we fine-tuned a model pre-trained on the synthetic dataset using a real dataset,
we did not see an increase in performance.
We further investigate the domain adaptation by mixing the synthetic and real datasets in different ratios per mini-batch for training the models.
We see an improvement in \gls(iou) of 1.93\% and 2.64\% on the baselines.
The same models show an increment of 0.8\% and 2.32\% with \gls{f1}.

As stated in \autoref{sec:contributions}, we have also conducted a comprehensive study on mixed training with different ratios and compared it with a fine-tuning or transfer learning approach.
We saw that mixed training performed better than fine-tuning the model.
A study was also conducted to see the domain randomization using only the chair dataset with different parameters.
We see a slight increase in the performance of the models with the addition of a new randomization component as in \autoref{sec:ablation-study-on-chairs}.
We now hypothesize that the real dataset has more images with no colored background, and since the test set is limited,
to begin with, the real dataset has less variance of real-world scenarios.

From the Ablation study on chairs with mixed training, we saw the best models(Pix2Vox++ and Pix2Vox) with \gls{iou} of 0.3774 and 0.3608,
which is an increase of 4.7\% and 6.92\% over models trained with only real chair dataset.
Similarly, we see an increment of 4.15\% and 6.37\% with \gls{f1}.
To conclude, synthetic data showed considerable performance improvement with blending real data for the 3D reconstruction tasks.

\section{\iftoggle{german}{Zukünftige Arbeiten}{Limitations}}\label{sec:limitations-of-game-engines2}

In this section, we discuss the limitations observed while conducting experiments for the thesis.

\subsection{Unity Game Engine}\label{subsec:unity-game-engine}
Unity game engine does not give control to the user for GPU usage.
It is entirely under the control of the framework, and the user can only hope for faster performance.
We observed that it took around 8hours to create 3000 images using the Ml-Image Synthesis library on a CPU.
Comparing this to Blenderproc, which takes 20 minutes for a single image on CPU, but proclaimed to create 3000 images in 1 hour with GPU usage.
Blenderproc has python as its underlying framework and hence can access GPUs easily

\subsection{Configuring synthetic dataset generation pipeline}\label{subsec:configuring-synthetic-dataset-generation-pipeline}
We had to try different configurations to try to get the optimum solution manually.
Moreover, since the shader layers used by unity are not differentiable, we can not train a model to optimize the input parameters like light intensity, light color, the darkness of shadow, texture colors, texture brightness, etc.
These parameters are randomized and not fine-tuned to increase the performance.
Though Blenderproc also does not do this, users may optimize these parameters since it uses python.

\subsection{High Definition Render Pipeline}\label{subsec:high-definition-render-pipeline}
Unity has introduced' High Definition Render Pipeline' (HDRP), which provides more realistic lighting and natural-looking scenes.
However, this pipeline is not cross-compatible, and the models/assets which are not generated for HDRP do not get rendered, as shown in \autoref{fig:hdrp}.
The shaders are not made public either and hence can not be assigned to models at runtime.
To use this pipeline, we will have to modify the models and make them compatible manually.

\section{\iftoggle{german}{Zukünftige Arbeiten}{Future Scope}}\label{sec:Future_scope}

In this section, we discuss future prospects of the generated dataset, and the ideas which were not implemented.

\subsection{Domain adaptation using Generative models}
A study can be conducted to see the effects of domain adaptation or style transfer using Generative models like CyceleGAN~\cite{CycleGAN2017} to see if the performance increases after conversion.
An attempt was made by creating a pipeline for CycelGAN, but we did not get a genuine style translation.
Unity provides a library to inference Deep Neural Networks\footnote{https://github.com/Unity-Technologies/barracuda-release}, which can be integrated with the camera for style transfer.
In this case, the user need not fine-tune the configuration for generating images and only needs to provide a few reference indoor room reference images for style optimization.

\subsection{A study on Style transfer with G-buffers}
The proposed S2R:3DFREE dataset provides various G-buffers like depth, normal, semantic and instance segmentation, and optical flows.
These buffer images can be used to enhance the photorealism of images, as demonstrated in ~\cite{Richter_2021}.
In the cited publication, the authors use G-buffer from the GTA-V game and enhance the photorealism, which can be used for training autonomous vehicles.
The dataset generated in this thesis can also be used similarly to train autonomous bots for indoor environments.

\subsection{Integrating the framework with Unity Cloud}
Unity has provided the functionality of integrating simulations on Amazon Web Service, and hence can be run on the cloud.
By achieving this, the user will not have to worry about running the data generator for days or weeks on the local machine.
Though, running models on AWS will come at a cost.

\subsection{Other forms of representations}
In this thesis, we focused on working with Voxel representation as indicated in \autoref{sec:Volumetric representation}.
It would be interesting to check if the performance increases significantly if the 3D model representation is changed to mesh or pointclouds.

\subsection{Increasing the synthetic dataset}
Though we have increased the dataset size by creating synthetically generated images.
The quantity is still limited.
For domain randomization to increase the performance, we might need a vast dataset.
Another dataset can be created using the same framework to check if more images are needed for domain randomization to show its impact.


