\chapter{\iftoggle{german}{Verwandte Arbeiten}{Related Work}}\label{ch:related_work}


\section{Indoor datasets}\label{ss:indoor scenes}

Indoor scene dataset has been in the rise with increasing interest in scene processing understanding ~\cite{dai2017scannet,Silberman2012IndoorSA,Xiao2013SUN3DAD,Hua2016SceneNNAS,Armeni20163DSP,chang2017matterport3d,Handa2016UnderstandingRI,InteriorNet18,li2021openrooms,zheng2020structured3d,Roberts2020HypersimAP,McCormac:etal:ICCV2017}.
Synthetic dataset is not something new in the world of machine learning.
As researchers realised the disadvantages of real dataset,focus was shifted to synthetic dataset.
While ~\cite{dai2017scannet,} are real-world datasets, ~\cite{Fu20203DFRONT3F,Handa2016UnderstandingRI,McCormac:etal:ICCV2017,Roberts2020HypersimAP} are synthetically produced.
The real-world dataset are gathered from live scans.
The synthetic dataset can either bet manually configured by a professional or automated by a programmer.

\subsubsection{Indoor synthetic datasets}
Alibaba group introduced 3D-FRONT~\cite{Fu20203DFRONT3F} which stands for 3D Furnished Rooms with layOuts and semaNTics dataset which comprises of
synthetic indoor scenes designed under the supervision of professionals.
It consists of 18,968 rooms and 13,151 textured furnitures.
SceneNet~\cite{McCormac:etal:ICCV2017} is a large collection of photorealistic images and trajectories.
This is discussed in detail in ~\ref{ss:SceneNet} section.

SunCG ~\cite{Song2017SemanticSC} was a key dataset for scene understanding.
The dataset contained over 45,000 variations of scenes with realistic room layout created manually.
Each scene was semantically labeld and also provided with volumetric ground truth data.
The datset has also been used for task like depth estimation, semantic scene completion, SLAM, indoor navigation, etc.
Unfortunately due to legal issues \footnote{https://futurism.com/tech-suing-facebook-princeton-data} the dataset has been made publically unavailable which has left a void in the field.

Structure3D ~\cite{zheng2020structured3d} is another impressive synthetic data for indoor scenes which introduced their own photorealistic renderer.
The dataset comprises 21,835 rooms in 3,500 scenes and 196k 2D-images rendered with photo-realism.
But the CAD models of the 3D furnitures which is used to populate the scenes is not made available to public.
And hence 3D reconstruction related tasks can not be performed.
It is also demonstrated that with combination of synthetic and real dataset deep learning task for room layout estimation improved performance on benchamark datasets.
This dataset is more focused on room layout estimation and not 3D reconstruction, but with few modification, it can be mapped to 3d furntiture reconstuction tasks.

Openrooms~\cite{li2021openrooms} use Scannet~\cite{dai2017scannet} as their layout foundation, retrieve corresponding models from shapenet~\cite{chang2015shapenet}
and then replace the CAD model with retrieved model with proper alignment.
They further add reflectance and illumination to compose photo-realistic images.
As of August 2021, only the dataset has been made public and not the generation tool or the CAD models.
Though the underlying concept of Openrooms is to convert existing scans into photo-realistic synthetic images, in terms of the output images we consider them our counterpart, as the framework can produce normals, depth maps, instancs segmentation and masks same as we do.

Hypermism ~\cite{Roberts2020HypersimAP} is Apple's repository for holistic indoor scene understanding.
It is a collection of synthetic scenes created with the help of professional artist.
~\cite{Evermotion} was the starting point for the dataset for which assets were purchased from ~\cite{TurboSquid}.
The dataset includes images, 3D assets, semantic instance segmentations, and a disentangled image representation with diffused lighting and shading.
Even though the 3d triangle meshes for each asset is available online, we have to purchase them to create custom dataset.
They also admit that the costs to generate the dataset is expensive \{approximately \$57K ~\cite{Roberts2020HypersimAP}\}.

InteriorNet ~\cite{InteriorNet18} claims to be a photo-realistic indoor scene simulator with realistic lighting and scenes which change over time.
The image dataset includes rgb, depth and semantic segmentations.
Along with images, they also provide synthesized realistic trajectories at video-frame rate with various motion patterns.
The simulator also supports scenes from ~\cite{McCormac:etal:ICCV2017} and ~\cite{Song2017SemanticSC} along with their own database.

Another simulated framework for visual research is House Of inteRactions (THOR) introduced in AI2-Thor ~\cite{kolve2019ai2thor}.
This is again a Agent focused photo-realistic dataset with the key factor being actionable objects so that agents can interact with the objects or manipulate them.
The underlying renderer for this framework is Unity game engine.
RoboThor ~\cite{Deitke2020RoboTHORAO} is built upon AI2-Thor which consists of real scenes and its corresponding synthetic equivalent.
This helps in the study of behavior of agents in real world when trained on synthetic data.

Habitat: A Platform for Embodied AI Research ~\cite{savva2019habitat}, is a photorealistic 3D simulation which can be used for training virtual agents for tasks like navigation, question answering, intruction foloowing.
The paper introduces Habitat-Sim which renders scenes from Matterport3d ~\cite{chang2017matterport3d}, Gibson ~\cite{xia2018gibson}, Replica ~\cite{Straub2019TheRD} and some other datasets.
The focus of the simulator is providing the agent with sensor data and allowing additional sensors as plugins.
At the foundation level Habitat-sim uses Magnum graphics
middleware library~\footnote{https://magnum.graphics/} which supports cross-platform on various hardware configuration.


\subsubsection{Tools to create synthetic datasets}

BlenderProc: Reducing the Reality Gap with Photorealistic Rendering ~\cite{denninger2019blenderproc}

NVIDIA developed a Deep learning Dataset Synthesizer (NDDS) ~\cite{to2018ndds} in the form of a plugin for Unreal Engine 4(UE4).
The plugin can synthesize images,per-pixel segmentation, depth, object 3D pose, 2D/3D bounding box, keypoints, and custom stencils.
It even supports domain randomisation of objects, lighting, camera position, poses and textures.
Leveraging the asynchronous-multithreaded frames, data was generated at high rates(50â€“100 Hz) for Falling Things (FAT) ~\cite{tremblay2018falling} dataset.

SynthDet: An end-to-end object detection pipeline using synthetic data

UnrealCV ~\cite{qiu2017unrealcv}

\todo{A table to explain diffs between various methods( only if 3D-FREE has more advantage)}

\section{State of the art for 3d-reconstruction}\label{ss:state_of_the_art}
Voxel based, mesh based
Pix2vox,OctNet,Mesh rcnn,Pixel2Mesh,Occupancy networks, etc

\section{Domain adaptation}\label{ss:domain_adaptation}
Discuss other methods used to mitigate domain shift
Example: distance learning, subspace matching

\subsection{GAN based style transfer}\label{ss:gan_based_styletransfer}

