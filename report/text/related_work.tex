\chapter{\iftoggle{german}{Verwandte Arbeiten}{Related Work}}\label{ch:related_work}

In this chapter we discuss the previous contributions to indoor synthetic datasets ~\ref{ss:Indoor synthetic datasets}, what tasks each of the dataset supports and some of the disadvantages of each dataset.
Further we discuss on the available tools which support creation of indoor dataset ~\ref{ss:tools to create synthetic}.

\section{Indoor datasets}\label{s:indoor dataset}

Indoor scene dataset has been in the rise with increasing interest in scene processing understanding ~\cite{dai2017scannet,Silberman2012IndoorSA,Xiao2013SUN3DAD,Hua2016SceneNNAS,Armeni20163DSP,chang2017matterport3d,Handa2016UnderstandingRI,InteriorNet18,li2021openrooms,zheng2020structured3d,Roberts2020HypersimAP,McCormac:etal:ICCV2017}.
Synthetic dataset is not something new in the world of machine learning.
As researchers realised the disadvantages of real dataset, focus was shifted to synthetic dataset.
While ~\cite{dai2017scannet,} are real-world datasets, ~\cite{Fu20203DFRONT3F,Handa2016UnderstandingRI,McCormac:etal:ICCV2017,Roberts2020HypersimAP} are synthetically produced.
The real-world dataset are gathered from live scans.
The synthetic dataset can either bet manually configured by a professional or automated by a programmer using a tool.

\subsection{Indoor synthetic datasets}\label{ss:Indoor synthetic datasets}
Alibaba group introduced 3D-FRONT~\cite{Fu20203DFRONT3F} which stands for 3D Furnished Rooms with layOuts and semaNTics dataset which comprises of
synthetic indoor scenes designed under the supervision of professionals.
It consists of 18,968 rooms and 13,151 textured furnitures.
SceneNet~\cite{McCormac:etal:ICCV2017} is a large collection of photorealistic images and trajectories.
This is discussed in detail in ~\ref{ss:SceneNet} section.

SunCG ~\cite{Song2017SemanticSC} was a key dataset for scene understanding.
The dataset contained over 45,000 variations of scenes with realistic room layout created manually.
Each scene was semantically labeled and also provided with volumetric ground truth data.
The datset has also been used for task like depth estimation, semantic scene completion, SLAM, indoor navigation, etc.
Unfortunately due to legal issues \footnote{https://futurism.com/tech-suing-facebook-princeton-data} the dataset has been made publically unavailable which has left a void in the field.

Structure3D ~\cite{zheng2020structured3d} is another impressive synthetic data for indoor scenes which introduced their own photorealistic renderer.
The dataset comprises 21,835 rooms in 3,500 scenes and 196k 2D-images rendered with photo-realism.
But the CAD models of the 3D furnitures which is used to populate the scenes is not made available to public.
And hence 3D reconstruction related tasks can not be performed.
It is also demonstrated that with combination of synthetic and real dataset deep learning task for room layout estimation improved performance on benchamark datasets.
This dataset is more focused on room layout estimation and not 3D reconstruction, but with few modification, it can be mapped to 3d furntiture reconstuction tasks.

Openrooms~\cite{li2021openrooms} use Scannet~\cite{dai2017scannet} as their layout foundation, retrieve corresponding models from shapenet~\cite{chang2015shapenet}
and then replace the CAD model with retrieved model with proper alignment.
They further add reflectance and illumination to compose photo-realistic images.
As of August 2021, only the dataset has been made public and not the generation tool or the CAD models.
Though the underlying concept of Openrooms is to convert existing scans into photo-realistic synthetic images, in terms of the output images we consider them our counterpart, as the framework can produce normals, depth maps, instancs segmentation and masks same as we do.

Hypermism ~\cite{Roberts2020HypersimAP} is Apple's repository for holistic indoor scene understanding.
It is a collection of synthetic scenes created with the help of professional artist.
~\cite{Evermotion} was the starting point for the dataset for which assets were purchased from ~\cite{TurboSquid}.
The dataset includes images, 3D assets, semantic instance segmentations, and a disentangled image representation with diffused lighting and shading.
Even though the 3d triangle meshes for each asset is available online, we have to purchase them to create custom dataset.
They also admit that the costs to generate the dataset is expensive \{approximately \$57K ~\cite{Roberts2020HypersimAP}\}.

InteriorNet ~\cite{InteriorNet18} claims to be a photo-realistic indoor scene simulator with realistic lighting and scenes which change over time.
The image dataset includes rgb, depth and semantic segmentations.
Along with images, they also provide synthesized realistic trajectories at video-frame rate with various motion patterns.
The simulator also supports scenes from ~\cite{McCormac:etal:ICCV2017} and ~\cite{Song2017SemanticSC} along with their own database.

Another simulated framework for visual research is House Of inteRactions (THOR) introduced in AI2-Thor ~\cite{kolve2019ai2thor}.
This is again a Agent focused photo-realistic dataset with the key factor being actionable objects so that agents can interact with the objects or manipulate them.
The underlying renderer for this framework is Unity game engine.
RoboThor ~\cite{Deitke2020RoboTHORAO} is built upon AI2-Thor which consists of real scenes and its corresponding synthetic equivalent.
This helps in the study of behavior of agents in real world when trained on synthetic data.

Habitat: A Platform for Embodied AI Research ~\cite{savva2019habitat}, is a photorealistic 3D simulation which can be used for training virtual agents for tasks like navigation, question answering, intruction foloowing.
The paper introduces Habitat-Sim which renders scenes from Matterport3d ~\cite{chang2017matterport3d}, Gibson ~\cite{xia2018gibson}, Replica ~\cite{Straub2019TheRD} and some other datasets.
The focus of the simulator is providing the agent with sensor data and allowing additional sensors as plugins.
At the foundation level Habitat-sim uses Magnum graphics
middleware library~\footnote{https://magnum.graphics/} which supports cross-platform on various hardware configuration.

\begin{figure}
\begin{tabular}{llll}
    3D FRONT & \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/3dfront_1} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/3dfront_2} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/3dfront_3}\\

    AI2THOR & \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/ai2thor_01} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/ai2thor_02} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/ai2thor_03}\\

    Hypersim & \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/hypersim_01} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/hypersim_02} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/hypersim_03}\\

    InteriorNet & \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/interiornet_01} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/interiornet_02} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/interiornet_03}\\

    OpenRooms & \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/openrooms_01} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/openrooms_02} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/openrooms_03}\\

    SceneNet & \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/scenenet_1} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/scenenet_2} &
    \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/scenenet_3}\\

\end{tabular}
\caption{A collection of photorealistic synthetic datasets. The first colomn of each row indicates the dataset name followed by randomly selected images from the same dataset.}
\label{fig:photorealistic images comparison}
\end{figure}

\subsection{Tools to create synthetic datasets}\label{ss:tools to create synthetic}

BlenderProc: Reducing the Reality Gap with Photorealistic Rendering ~\cite{denninger2019blenderproc} is a python based pipeline to create synthetic dataset.
As the name suggests, the key underlying framework is Blender ~\cite{blender}, which is a 3D modelling and rendering package.
Like most of the synthetic data generation tools it provides with rgb, depth maps, normals, semantic segmentation.
Figure ~\ref{fig:Blenderproc samples} is a collection of sample images generated using BlenderPoc.
As far as we searched for synthetic data generation tools, Blenderproc supports maximum number of existing dataset.
Ikea~\cite{Lim2013ParsingIO}, Pix3d~\cite{pix3d}, Shapenet~\cite{chang2015shapenet},3DFront~\cite{Fu20203DFRONT3F} Replica~\cite{Straub2019TheRD}, SunCG~\cite{Xiao2013SUN3DAD} are some of the popular dataset it supports.
It also has some combinations of these datasets like ShapeNet with SunCG or SceneNet.
Even though it supports rendering of Pix3D dataset, the model is rendered without any background.
One extreme advantage of this toolkit, is that it is opensource and hence a wider open community to contribute.

\begin{figure}
    \begin{tabular}{llll}
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_1} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_depth_1} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_instance_1} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_normal_1}\\


        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_2} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_depth_2} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_instance_2} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_normal_2}\\

        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_3} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_depth_3} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_instance_3} &
        \includegraphics[width=.2\linewidth,valign=m]{/Users/apple/OVGU/Thesis/code/3dReconstruction/report/images/realistic_images_relatedwork/blenderproc_normal_3}\\

    \end{tabular}
    \caption{Sample images created from BlenderProc using SceneNet dataset.(Left to right) RGB images, Depth Maps, Instance Segmenatations and Normals. Each row is an independent sample.}
    \label{fig:Blenderproc samples}
\end{figure}

NVIDIA developed a Deep learning Dataset Synthesizer (NDDS) ~\cite{to2018ndds} in the form of a plugin for Unreal Engine 4(UE4).
The plugin can synthesize images,per-pixel segmentation, depth, object 3D pose, 2D/3D bounding box, keypoints, and custom stencils.
It even supports domain randomisation of objects, lighting, camera position, poses and textures.
Leveraging the asynchronous-multithreaded frames, data was generated at high rates(50\â€“100 Hz) for Falling Things (FAT) ~\cite{tremblay2018falling} dataset.

"SynthDet: An end-to-end object detection pipeline using synthetic data" ~\cite{synthdet2020}, is an open source project supported by Unity technologies using Unity Engine.
This is an ML pipeline which uses 63 categories of common objects(example: cereal box, cady, cartons, etc) as synthetic objects to generate 2D bounding boxes.
This project was highly influenced by ~\cite{hinterstoisser2019annotation}, where in they use synthetic 3D models with randomised background to generate synthetic dataset for object detection task.
With domain randomisation the author of ~\cite{hinterstoisser2019annotation} proved that no real data or mixed training is required for Deep Learning model to perform significantly.

UnrealCV ~\cite{qiu2017unrealcv} is open source project built upon Unreal Engine 4 (UE4) ~\cite{unrealengine}).
It supports some pre-built indoor architectures available on the asset store, which are unfortunately paid versions.
Along with this limitation, the plugin only creates depthmaps, normals and segmentation masks, with no mapping to 3D models.
Another tool based on Unreal engine is UnrealROX+\cite{martinezgonzalez2021unrealrox}.
This pluggin design is based on UnrealCV and provides similar data.

\todo{A table to explain diffs between various methods( only if 3D-FREE has more advantage)}

\section{State of the art for 3d-reconstruction}\label{ss:state_of_the_art}
Voxel based, mesh based
Pix2vox,OctNet,Mesh rcnn,Pixel2Mesh,Occupancy networks, etc

\section{Domain Randomisation}\label{s:domain randomisation}
Discuss other methods used to mitigate domain shift
Example: distance learning, subspace matching

\section{GAN based style transfer}\label{s:gan_based_styletransfer}

