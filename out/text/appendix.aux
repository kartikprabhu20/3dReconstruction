\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{Appendix}{69}{appendix*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Study with F-Score}{69}{section.A.1}\protected@file@percent }
\newlabel{sec:study-with-f-score}{{A.1}{69}{Study with F-Score}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Baseline}{69}{subsection.A.1.1}\protected@file@percent }
\newlabel{subsec:baseline_dice}{{A.1.1}{69}{Baseline}{subsection.A.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Bar plot for the \gls {f1} for baselines trained on real and synthetic datasets, with and without 2D augmentation. We see that ~\gls {free} does not perform adequately on its own. \gls {s2rv2} contributes slightly better than \gls {s2rv1}.\relax }}{69}{figure.caption.60}\protected@file@percent }
\newlabel{fig:baseline_dice1}{{A.1}{69}{Bar plot for the \gls {f1} for baselines trained on real and synthetic datasets, with and without 2D augmentation. We see that ~\gls {free} does not perform adequately on its own. \gls {s2rv2} contributes slightly better than \gls {s2rv1}.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Fine-Tuning}{70}{subsection.A.1.2}\protected@file@percent }
\newlabel{subsec:fine-tuning-dice}{{A.1.2}{70}{Fine-Tuning}{subsection.A.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Bar plot for the \gls {f1} for baseline models(Pix2Vox++ and Pix2Vox) trained on synthetic and fine-tuned with real dataset. We see that even after fine-tuning both the models do not perform as good as models trained on only real dataset.\relax }}{70}{figure.caption.61}\protected@file@percent }
\newlabel{fig:finetuning_dice1}{{A.2}{70}{Bar plot for the \gls {f1} for baseline models(Pix2Vox++ and Pix2Vox) trained on synthetic and fine-tuned with real dataset. We see that even after fine-tuning both the models do not perform as good as models trained on only real dataset.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.3}Mixed training}{70}{subsection.A.1.3}\protected@file@percent }
\newlabel{subsec:mixed-training-dice}{{A.1.3}{70}{Mixed training}{subsection.A.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Parallel coordinate plot for the \gls {f1} for baseline pix2vox++ trained on (\gls {s2rv1}, \gls {s2rv2}) and fine-tuned with pix3d. The categories are listed along with the number of images. The performance of {pix2vox++} mixed with both the synthetic dataset is less than model trained on only pix3d, for majority of the categories.\relax }}{71}{figure.caption.62}\protected@file@percent }
\newlabel{fig:finetuning_dice2}{{A.3}{71}{Parallel coordinate plot for the \gls {f1} for baseline pix2vox++ trained on (\gls {s2rv1}, \gls {s2rv2}) and fine-tuned with pix3d. The categories are listed along with the number of images. The performance of \texbf {pix2vox++} mixed with both the synthetic dataset is less than model trained on only pix3d, for majority of the categories.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Parallel coordinate plot for the \gls {f1} for baseline {pix2vox} trained on (\gls {s2rv1}, \gls {s2rv2}) and fine-tuned with pix3d. The categories are listed along with the number of images. The performance of pix2vox mixed with both the synthetic dataset is less than model trained on only pix3d, for majority the categories.\relax }}{71}{figure.caption.63}\protected@file@percent }
\newlabel{fig:finetuning_dice3}{{A.4}{71}{Parallel coordinate plot for the \gls {f1} for baseline \texbf {pix2vox} trained on (\gls {s2rv1}, \gls {s2rv2}) and fine-tuned with pix3d. The categories are listed along with the number of images. The performance of pix2vox mixed with both the synthetic dataset is less than model trained on only pix3d, for majority the categories.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces Bar plot for the \gls {f1} for baselines trained on different ratios of synthetic and real dataset. (left)Mixed training on Pix2Vox++, (right)Mixed training on Pix2Vox. In both cases we see a slight increase in \gls {f1} with addition of real data, and a gradual decrease till it reaches 100\% real data\relax }}{72}{figure.caption.64}\protected@file@percent }
\newlabel{fig:mixed_dice1}{{A.5}{72}{Bar plot for the \gls {f1} for baselines trained on different ratios of synthetic and real dataset. (left)Mixed training on Pix2Vox++, (right)Mixed training on Pix2Vox. In both cases we see a slight increase in \gls {f1} with addition of real data, and a gradual decrease till it reaches 100\% real data\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces Parallel coordinate plot for the \gls {f1} for baseline {pix2vox++} trained on 50\% of mixed dataset(\gls {s2rv1}, \gls {s2rv2}) and with pix3d. The categories are listed along with the number of images. The performance of pix2vox++ mixed with both the synthetic dataset is more than model trained on only pix3d, for most of the categories. \relax }}{72}{figure.caption.65}\protected@file@percent }
\newlabel{fig:mixed_dice2}{{A.6}{72}{Parallel coordinate plot for the \gls {f1} for baseline \texbf {pix2vox++} trained on 50\% of mixed dataset(\gls {s2rv1}, \gls {s2rv2}) and with pix3d. The categories are listed along with the number of images. The performance of pix2vox++ mixed with both the synthetic dataset is more than model trained on only pix3d, for most of the categories. \relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces Parallel coordinate plot for the \gls {f1} for baseline {pix2vox} trained on 50\% of mixed dataset(\gls {s2rv1}, \gls {s2rv2}) and with pix3d. The categories are listed along with the number of images. The performance of pix2vox mixed with both the synthetic dataset is more than model trained on only pix3d, for all the categories.\relax }}{73}{figure.caption.66}\protected@file@percent }
\newlabel{fig:mixed_dice3}{{A.7}{73}{Parallel coordinate plot for the \gls {f1} for baseline \texbf {pix2vox} trained on 50\% of mixed dataset(\gls {s2rv1}, \gls {s2rv2}) and with pix3d. The categories are listed along with the number of images. The performance of pix2vox mixed with both the synthetic dataset is more than model trained on only pix3d, for all the categories.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.4}Ablation study on chairs}{73}{subsection.A.1.4}\protected@file@percent }
\newlabel{subsec:ablation-study-on-chairs}{{A.1.4}{73}{Ablation study on chairs}{subsection.A.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Additional outputs}{73}{section.A.2}\protected@file@percent }
\newlabel{sec:additional-outputs}{{A.2}{73}{Additional outputs}{section.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces Bar plot for the \gls {f1} for baseline trained on chair dataset with different domain randomization parameters and tested on real dataset. We see a dip in performance near textureless dataset, but it gradually increases with addition of domain randomization parameter.\relax }}{74}{figure.caption.67}\protected@file@percent }
\newlabel{fig:ablation_dice1}{{A.8}{74}{Bar plot for the \gls {f1} for baseline trained on chair dataset with different domain randomization parameters and tested on real dataset. We see a dip in performance near textureless dataset, but it gradually increases with addition of domain randomization parameter.\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.9}{\ignorespaces Bar plot for the \gls {f1} for baseline trained by mixing chair dataset from real and synthetic dataset with ratio of 50\%. Observe that the {IoU} is consistent for all types of randomization proving that mixed training negates loss from randomization.\relax }}{74}{figure.caption.68}\protected@file@percent }
\newlabel{fig:ablation_dice2}{{A.9}{74}{Bar plot for the \gls {f1} for baseline trained by mixing chair dataset from real and synthetic dataset with ratio of 50\%. Observe that the \fls {IoU} is consistent for all types of randomization proving that mixed training negates loss from randomization.\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.10}{\ignorespaces 3D reconstruction outputs for models trained on \textbf  {only real dataset}, and \textbf  {only synthetic dataset}. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D(real dataset). Output3-4: Pix2Vox++ and Pix2Vox trained with only \gls {s2rv2} synthetic dataset. This corresponds to the bad \gls {iou} when trained on only synthetic dataset.\relax }}{75}{figure.caption.69}\protected@file@percent }
\newlabel{fig:baseline_more_images1}{{A.10}{75}{3D reconstruction outputs for models trained on \textbf {only real dataset}, and \textbf {only synthetic dataset}. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D(real dataset). Output3-4: Pix2Vox++ and Pix2Vox trained with only \gls {s2rv2} synthetic dataset. This corresponds to the bad \gls {iou} when trained on only synthetic dataset.\relax }{figure.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.11}{\ignorespaces 3D reconstruction outputs for models trained on real dataset and synthetic datasets with \textbf  {fine-tuning}. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D(real dataset). Output3-4: Pix2Vox++ and Pix2Vox pre-trained with only \gls {s2rv2} synthetic dataset and then fine-tuned with Pix3d. The reconstruction is better than models trained on only synthetic dataset.\relax }}{76}{figure.caption.70}\protected@file@percent }
\newlabel{fig:finetuning_more_images1}{{A.11}{76}{3D reconstruction outputs for models trained on real dataset and synthetic datasets with \textbf {fine-tuning}. Output1-2: Pix2Vox++ and Pix2Vox trained on Pix3D(real dataset). Output3-4: Pix2Vox++ and Pix2Vox pre-trained with only \gls {s2rv2} synthetic dataset and then fine-tuned with Pix3d. The reconstruction is better than models trained on only synthetic dataset.\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.12}{\ignorespaces 3D reconstruction outputs for best \textbf  {mixed training}(50\% per mini-batch) models. Output1-2: Pix2Vox++ and Pix2Vox mixed trained with \gls {s2rv1}. Output3-4:Pix2Vox++ and Pix2Vox mixed trained with \gls {s2rv2}\relax }}{77}{figure.caption.71}\protected@file@percent }
\newlabel{fig:mixed_more_images1}{{A.12}{77}{3D reconstruction outputs for best \textbf {mixed training}(50\% per mini-batch) models. Output1-2: Pix2Vox++ and Pix2Vox mixed trained with \gls {s2rv1}. Output3-4:Pix2Vox++ and Pix2Vox mixed trained with \gls {s2rv2}\relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Additional attempts}{78}{section.A.3}\protected@file@percent }
\newlabel{sec:some-failed-attempts}{{A.3}{78}{Additional attempts}{section.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3.1}High Definition Render Pipeline(HDRP)}{78}{subsection.A.3.1}\protected@file@percent }
\newlabel{subsec:high-definition-render-pipeline(hdrp)}{{A.3.1}{78}{High Definition Render Pipeline(HDRP)}{subsection.A.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.13}{\ignorespaces Sample images for models rendered in High Definition Render Pipeline of Unity\relax }}{78}{figure.caption.72}\protected@file@percent }
\newlabel{fig:hdrp}{{A.13}{78}{Sample images for models rendered in High Definition Render Pipeline of Unity\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3.2}Empty image test}{78}{subsection.A.3.2}\protected@file@percent }
\newlabel{subsec:empty-image-test}{{A.3.2}{78}{Empty image test}{subsection.A.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.14}{\ignorespaces 3D reconstruction outputs for best \textbf  {mixed training}(50\% per mini-batch) models for empty rooms. We do not see a detailed reconstruction, but some of them resemble furniture category.\relax }}{79}{figure.caption.73}\protected@file@percent }
\newlabel{fig:empty_images1}{{A.14}{79}{3D reconstruction outputs for best \textbf {mixed training}(50\% per mini-batch) models for empty rooms. We do not see a detailed reconstruction, but some of them resemble furniture category.\relax }{figure.caption.73}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Samples from Survey}{80}{section.A.4}\protected@file@percent }
\newlabel{sec:samples-from-survey}{{A.4}{80}{Samples from Survey}{section.A.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.15}{\ignorespaces A sample question used for survey section 1. For section 1, participants were asked to select 'Real' or 'Not real' for 27 images, 3 images per dataset.\relax }}{80}{figure.caption.74}\protected@file@percent }
\newlabel{fig:survey_sample_section1}{{A.15}{80}{A sample question used for survey section 1. For section 1, participants were asked to select 'Real' or 'Not real' for 27 images, 3 images per dataset.\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.16}{\ignorespaces A sample question used for survey section 2. For section 2, participants were asked rate the 27 images from 1 to 10. We had 3 images per dataset.\relax }}{81}{figure.caption.75}\protected@file@percent }
\newlabel{fig:survey_sample_section2}{{A.16}{81}{A sample question used for survey section 2. For section 2, participants were asked rate the 27 images from 1 to 10. We had 3 images per dataset.\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.17}{\ignorespaces A sample question used for survey section 3. For section 3, the users were shown 9 images together and asked to rank them on descending order of photorealism. \relax }}{82}{figure.caption.76}\protected@file@percent }
\newlabel{fig:survey_sample_section3}{{A.17}{82}{A sample question used for survey section 3. For section 3, the users were shown 9 images together and asked to rank them on descending order of photorealism. \relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Tables}{83}{section.A.5}\protected@file@percent }
\newlabel{sec:tables}{{A.5}{83}{Tables}{section.A.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox++. The models trained on synthetic data is significantly bad when tested on real-data.\relax }}{83}{table.caption.77}\protected@file@percent }
\newlabel{tab:baseline_categories_all_iou_pix2voxpp}{{A.1}{83}{Table represents \gls {iou} values for each category when trained on Pix2Vox++. The models trained on synthetic data is significantly bad when tested on real-data.\relax }{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox. The models trained on synthetic data is significantly bad when tested on real-data.\relax }}{83}{table.caption.78}\protected@file@percent }
\newlabel{tab:baseline_categories_all_iou_pix2vox}{{A.2}{83}{Table represents \gls {iou} values for each category when trained on Pix2Vox. The models trained on synthetic data is significantly bad when tested on real-data.\relax }{table.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox++. The models trained on synthetic data and then fine-tuned with real data. No improvement is seen with fine-tuning.\relax }}{83}{table.caption.79}\protected@file@percent }
\newlabel{tab:finetuning_categories_all_iou_pix2voxpp}{{A.3}{83}{Table represents \gls {iou} values for each category when trained on Pix2Vox++. The models trained on synthetic data and then fine-tuned with real data. No improvement is seen with fine-tuning.\relax }{table.caption.79}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox. The models trained on synthetic dataand fine-tuned with real-data. A slight improvement is seen in some of the categories and the average \gls {iou}\relax }}{84}{table.caption.80}\protected@file@percent }
\newlabel{tab:finetuning_categories_all_iou_pix2vox}{{A.4}{84}{Table represents \gls {iou} values for each category when trained on Pix2Vox. The models trained on synthetic dataand fine-tuned with real-data. A slight improvement is seen in some of the categories and the average \gls {iou}\relax }{table.caption.80}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox++. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }}{84}{table.caption.81}\protected@file@percent }
\newlabel{tab:mixed_categories_all_iou_pix2voxpp}{{A.5}{84}{Table represents \gls {iou} values for each category when trained on Pix2Vox++. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }{table.caption.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }}{84}{table.caption.82}\protected@file@percent }
\newlabel{tab:mixed_categories_all_iou_pix2vox}{{A.6}{84}{Table represents \gls {iou} values for each category when trained on Pix2Vox. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }{table.caption.82}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.7}{\ignorespaces Table represents \gls {iou} values for each of the dataset created with different domain randomization component. The last 2 colums is mixed training with 50\% of mini-batches being Pix3D. We see that mixed training consistently increases the performance of the models.\relax }}{85}{table.caption.83}\protected@file@percent }
\newlabel{tab:ablation1}{{A.7}{85}{Table represents \gls {iou} values for each of the dataset created with different domain randomization component. The last 2 colums is mixed training with 50\% of mini-batches being Pix3D. We see that mixed training consistently increases the performance of the models.\relax }{table.caption.83}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.8}{\ignorespaces Table represents \gls {f1} values for each category when trained on Pix2Vox++. The models trained on synthetic data is significantly bad when tested on real-data.\relax }}{85}{table.caption.84}\protected@file@percent }
\newlabel{tab:baseline_categories_all_f1_pix2voxpp}{{A.8}{85}{Table represents \gls {f1} values for each category when trained on Pix2Vox++. The models trained on synthetic data is significantly bad when tested on real-data.\relax }{table.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.9}{\ignorespaces Table represents \gls {f1} values for each category when trained on Pix2Vox. The models trained on synthetic data is significantly bad when tested on real-data.\relax }}{85}{table.caption.85}\protected@file@percent }
\newlabel{tab:baseline_categories_all_f1_pix2vox}{{A.9}{85}{Table represents \gls {f1} values for each category when trained on Pix2Vox. The models trained on synthetic data is significantly bad when tested on real-data.\relax }{table.caption.85}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.10}{\ignorespaces Table represents \gls {f1} values for each category when trained on Pix2Vox++. The models trained on synthetic data and then fine-tuned with real data. No improvement is seen with fine-tuning.\relax }}{85}{table.caption.86}\protected@file@percent }
\newlabel{tab:finetuning_categories_all_f1_pix2voxpp}{{A.10}{85}{Table represents \gls {f1} values for each category when trained on Pix2Vox++. The models trained on synthetic data and then fine-tuned with real data. No improvement is seen with fine-tuning.\relax }{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.11}{\ignorespaces Table represents \gls {f1} values for each category when trained on Pix2Vox. The models trained on synthetic dataand fine-tuned with real-data. A slight improvement is seen in some of the categories and the average \gls {iou}\relax }}{86}{table.caption.87}\protected@file@percent }
\newlabel{tab:finetuning_categories_all_f1_pix2vox}{{A.11}{86}{Table represents \gls {f1} values for each category when trained on Pix2Vox. The models trained on synthetic dataand fine-tuned with real-data. A slight improvement is seen in some of the categories and the average \gls {iou}\relax }{table.caption.87}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.12}{\ignorespaces Table represents \gls {f1} values for each category when trained on Pix2Vox++. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }}{86}{table.caption.88}\protected@file@percent }
\newlabel{tab:mixed_categories_all_f1_pix2voxpp}{{A.12}{86}{Table represents \gls {f1} values for each category when trained on Pix2Vox++. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }{table.caption.88}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.13}{\ignorespaces Table represents \gls {iou} values for each category when trained on Pix2Vox. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }}{86}{table.caption.89}\protected@file@percent }
\newlabel{tab:mixed_categories_all_f1_pix2vox}{{A.13}{86}{Table represents \gls {iou} values for each category when trained on Pix2Vox. The first column indicates the synthetic dataset and ratio of real data used in each mini-batch.\relax }{table.caption.89}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.14}{\ignorespaces Table represents \gls {f1} values for each of the dataset created with different domain randomization component. The last 2 colums is mixed training with 50\% of mini-batches being Pix3D. We see that mixed training consistently increases the performance of the models.\relax }}{87}{table.caption.90}\protected@file@percent }
\newlabel{tab:ablation1_f1}{{A.14}{87}{Table represents \gls {f1} values for each of the dataset created with different domain randomization component. The last 2 colums is mixed training with 50\% of mini-batches being Pix3D. We see that mixed training consistently increases the performance of the models.\relax }{table.caption.90}{}}
\@setckpt{text/appendix}{
\setcounter{page}{88}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{nag@c}{51}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{14}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{4}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{0}
\setcounter{mn@abspage}{112}
\setcounter{lstnumber}{1}
\setcounter{float@type}{16}
\setcounter{nlinenum}{0}
\setcounter{su@anzahl}{0}
\setcounter{DTLrowi}{0}
\setcounter{DTLrowii}{0}
\setcounter{DTLrowiii}{0}
\setcounter{DTLrow}{0}
\setcounter{Item}{24}
\setcounter{Hfootnote}{4}
\setcounter{bookmark@seq@number}{105}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{tcb@cnt@definition}{0}
\setcounter{tcb@cnt@theorem}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
